{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.967904156189912,
  "eval_steps": 500,
  "global_step": 8400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005916284573287975,
      "grad_norm": 1.1753195524215698,
      "learning_rate": 0.0001998580721466588,
      "loss": 6.1845,
      "step": 10
    },
    {
      "epoch": 0.01183256914657595,
      "grad_norm": 0.41686296463012695,
      "learning_rate": 0.00019962152572442342,
      "loss": 0.334,
      "step": 20
    },
    {
      "epoch": 0.017748853719863924,
      "grad_norm": 0.2748197019100189,
      "learning_rate": 0.00019938497930218808,
      "loss": 0.2768,
      "step": 30
    },
    {
      "epoch": 0.0236651382931519,
      "grad_norm": 0.1670396476984024,
      "learning_rate": 0.0001991484328799527,
      "loss": 0.2689,
      "step": 40
    },
    {
      "epoch": 0.029581422866439874,
      "grad_norm": 0.17428931593894958,
      "learning_rate": 0.00019891188645771734,
      "loss": 0.2305,
      "step": 50
    },
    {
      "epoch": 0.03549770743972785,
      "grad_norm": 0.16737571358680725,
      "learning_rate": 0.000198675340035482,
      "loss": 0.24,
      "step": 60
    },
    {
      "epoch": 0.04141399201301583,
      "grad_norm": 0.17053695023059845,
      "learning_rate": 0.0001984387936132466,
      "loss": 0.2289,
      "step": 70
    },
    {
      "epoch": 0.0473302765863038,
      "grad_norm": 0.1927040070295334,
      "learning_rate": 0.00019820224719101123,
      "loss": 0.2355,
      "step": 80
    },
    {
      "epoch": 0.053246561159591775,
      "grad_norm": 0.18730428814888,
      "learning_rate": 0.00019796570076877589,
      "loss": 0.222,
      "step": 90
    },
    {
      "epoch": 0.05916284573287975,
      "grad_norm": 0.17941232025623322,
      "learning_rate": 0.00019772915434654052,
      "loss": 0.1988,
      "step": 100
    },
    {
      "epoch": 0.06507913030616773,
      "grad_norm": 0.2031998336315155,
      "learning_rate": 0.00019749260792430515,
      "loss": 0.2181,
      "step": 110
    },
    {
      "epoch": 0.0709954148794557,
      "grad_norm": 0.1674601435661316,
      "learning_rate": 0.00019725606150206978,
      "loss": 0.2141,
      "step": 120
    },
    {
      "epoch": 0.07691169945274368,
      "grad_norm": 0.19112984836101532,
      "learning_rate": 0.00019701951507983443,
      "loss": 0.2224,
      "step": 130
    },
    {
      "epoch": 0.08282798402603166,
      "grad_norm": 0.288748174905777,
      "learning_rate": 0.00019678296865759907,
      "loss": 0.2313,
      "step": 140
    },
    {
      "epoch": 0.08874426859931962,
      "grad_norm": 0.20942305028438568,
      "learning_rate": 0.0001965464222353637,
      "loss": 0.194,
      "step": 150
    },
    {
      "epoch": 0.0946605531726076,
      "grad_norm": 0.17547893524169922,
      "learning_rate": 0.00019630987581312835,
      "loss": 0.2091,
      "step": 160
    },
    {
      "epoch": 0.10057683774589558,
      "grad_norm": 0.1593792885541916,
      "learning_rate": 0.00019607332939089298,
      "loss": 0.2035,
      "step": 170
    },
    {
      "epoch": 0.10649312231918355,
      "grad_norm": 0.15888138115406036,
      "learning_rate": 0.00019583678296865761,
      "loss": 0.2118,
      "step": 180
    },
    {
      "epoch": 0.11240940689247153,
      "grad_norm": 0.18389438092708588,
      "learning_rate": 0.00019560023654642224,
      "loss": 0.2214,
      "step": 190
    },
    {
      "epoch": 0.1183256914657595,
      "grad_norm": 0.15423855185508728,
      "learning_rate": 0.00019536369012418688,
      "loss": 0.2208,
      "step": 200
    },
    {
      "epoch": 0.12424197603904748,
      "grad_norm": 0.17931252717971802,
      "learning_rate": 0.0001951271437019515,
      "loss": 0.2094,
      "step": 210
    },
    {
      "epoch": 0.13015826061233546,
      "grad_norm": 0.16401168704032898,
      "learning_rate": 0.00019489059727971614,
      "loss": 0.1998,
      "step": 220
    },
    {
      "epoch": 0.13607454518562342,
      "grad_norm": 0.18580178916454315,
      "learning_rate": 0.0001946540508574808,
      "loss": 0.1967,
      "step": 230
    },
    {
      "epoch": 0.1419908297589114,
      "grad_norm": 0.35297951102256775,
      "learning_rate": 0.00019441750443524542,
      "loss": 0.1902,
      "step": 240
    },
    {
      "epoch": 0.14790711433219939,
      "grad_norm": 0.168944850564003,
      "learning_rate": 0.00019418095801301005,
      "loss": 0.1958,
      "step": 250
    },
    {
      "epoch": 0.15382339890548735,
      "grad_norm": 0.1913520246744156,
      "learning_rate": 0.0001939444115907747,
      "loss": 0.2056,
      "step": 260
    },
    {
      "epoch": 0.15973968347877532,
      "grad_norm": 0.19522392749786377,
      "learning_rate": 0.00019370786516853934,
      "loss": 0.2147,
      "step": 270
    },
    {
      "epoch": 0.1656559680520633,
      "grad_norm": 0.23896348476409912,
      "learning_rate": 0.00019347131874630397,
      "loss": 0.2047,
      "step": 280
    },
    {
      "epoch": 0.17157225262535128,
      "grad_norm": 0.17035023868083954,
      "learning_rate": 0.0001932347723240686,
      "loss": 0.1927,
      "step": 290
    },
    {
      "epoch": 0.17748853719863925,
      "grad_norm": 0.21285293996334076,
      "learning_rate": 0.00019299822590183326,
      "loss": 0.2078,
      "step": 300
    },
    {
      "epoch": 0.18340482177192724,
      "grad_norm": 0.1820935308933258,
      "learning_rate": 0.0001927616794795979,
      "loss": 0.2071,
      "step": 310
    },
    {
      "epoch": 0.1893211063452152,
      "grad_norm": 0.1617012470960617,
      "learning_rate": 0.0001925251330573625,
      "loss": 0.1936,
      "step": 320
    },
    {
      "epoch": 0.19523739091850317,
      "grad_norm": 0.17522133886814117,
      "learning_rate": 0.00019228858663512715,
      "loss": 0.2099,
      "step": 330
    },
    {
      "epoch": 0.20115367549179117,
      "grad_norm": 0.1689951866865158,
      "learning_rate": 0.00019205204021289178,
      "loss": 0.2054,
      "step": 340
    },
    {
      "epoch": 0.20706996006507913,
      "grad_norm": 0.1546987146139145,
      "learning_rate": 0.00019181549379065641,
      "loss": 0.1904,
      "step": 350
    },
    {
      "epoch": 0.2129862446383671,
      "grad_norm": 0.24142815172672272,
      "learning_rate": 0.00019157894736842104,
      "loss": 0.2072,
      "step": 360
    },
    {
      "epoch": 0.21890252921165507,
      "grad_norm": 0.1541711986064911,
      "learning_rate": 0.0001913424009461857,
      "loss": 0.2051,
      "step": 370
    },
    {
      "epoch": 0.22481881378494306,
      "grad_norm": 0.14978428184986115,
      "learning_rate": 0.00019110585452395033,
      "loss": 0.1971,
      "step": 380
    },
    {
      "epoch": 0.23073509835823103,
      "grad_norm": 0.16841082274913788,
      "learning_rate": 0.00019086930810171496,
      "loss": 0.2,
      "step": 390
    },
    {
      "epoch": 0.236651382931519,
      "grad_norm": 0.14976923167705536,
      "learning_rate": 0.00019063276167947962,
      "loss": 0.197,
      "step": 400
    },
    {
      "epoch": 0.242567667504807,
      "grad_norm": 0.16401445865631104,
      "learning_rate": 0.00019039621525724425,
      "loss": 0.1944,
      "step": 410
    },
    {
      "epoch": 0.24848395207809496,
      "grad_norm": 0.1541743278503418,
      "learning_rate": 0.00019015966883500888,
      "loss": 0.1888,
      "step": 420
    },
    {
      "epoch": 0.25440023665138295,
      "grad_norm": 0.20403505861759186,
      "learning_rate": 0.0001899231224127735,
      "loss": 0.1901,
      "step": 430
    },
    {
      "epoch": 0.2603165212246709,
      "grad_norm": 0.15036222338676453,
      "learning_rate": 0.00018968657599053817,
      "loss": 0.1924,
      "step": 440
    },
    {
      "epoch": 0.2662328057979589,
      "grad_norm": 0.1411740779876709,
      "learning_rate": 0.00018945002956830277,
      "loss": 0.1878,
      "step": 450
    },
    {
      "epoch": 0.27214909037124685,
      "grad_norm": 0.14667508006095886,
      "learning_rate": 0.0001892134831460674,
      "loss": 0.1917,
      "step": 460
    },
    {
      "epoch": 0.2780653749445348,
      "grad_norm": 0.2585884928703308,
      "learning_rate": 0.00018897693672383206,
      "loss": 0.215,
      "step": 470
    },
    {
      "epoch": 0.2839816595178228,
      "grad_norm": 0.20198975503444672,
      "learning_rate": 0.0001887403903015967,
      "loss": 0.1992,
      "step": 480
    },
    {
      "epoch": 0.2898979440911108,
      "grad_norm": 0.15403775870800018,
      "learning_rate": 0.00018850384387936132,
      "loss": 0.1956,
      "step": 490
    },
    {
      "epoch": 0.29581422866439877,
      "grad_norm": 0.19970424473285675,
      "learning_rate": 0.00018826729745712598,
      "loss": 0.2037,
      "step": 500
    },
    {
      "epoch": 0.30173051323768674,
      "grad_norm": 0.17667630314826965,
      "learning_rate": 0.0001880307510348906,
      "loss": 0.1937,
      "step": 510
    },
    {
      "epoch": 0.3076467978109747,
      "grad_norm": 0.15852423012256622,
      "learning_rate": 0.00018779420461265524,
      "loss": 0.1878,
      "step": 520
    },
    {
      "epoch": 0.31356308238426267,
      "grad_norm": 0.2587567865848541,
      "learning_rate": 0.00018755765819041987,
      "loss": 0.2144,
      "step": 530
    },
    {
      "epoch": 0.31947936695755064,
      "grad_norm": 0.21328945457935333,
      "learning_rate": 0.00018732111176818453,
      "loss": 0.1971,
      "step": 540
    },
    {
      "epoch": 0.32539565153083866,
      "grad_norm": 0.23585668206214905,
      "learning_rate": 0.00018708456534594916,
      "loss": 0.1879,
      "step": 550
    },
    {
      "epoch": 0.3313119361041266,
      "grad_norm": 0.1639442890882492,
      "learning_rate": 0.0001868480189237138,
      "loss": 0.2158,
      "step": 560
    },
    {
      "epoch": 0.3372282206774146,
      "grad_norm": 0.16282491385936737,
      "learning_rate": 0.00018661147250147845,
      "loss": 0.1943,
      "step": 570
    },
    {
      "epoch": 0.34314450525070256,
      "grad_norm": 0.16966742277145386,
      "learning_rate": 0.00018637492607924305,
      "loss": 0.195,
      "step": 580
    },
    {
      "epoch": 0.3490607898239905,
      "grad_norm": 0.18126149475574493,
      "learning_rate": 0.00018613837965700768,
      "loss": 0.2062,
      "step": 590
    },
    {
      "epoch": 0.3549770743972785,
      "grad_norm": 0.14590437710285187,
      "learning_rate": 0.00018590183323477234,
      "loss": 0.1861,
      "step": 600
    },
    {
      "epoch": 0.36089335897056646,
      "grad_norm": 0.1814342588186264,
      "learning_rate": 0.00018566528681253697,
      "loss": 0.1944,
      "step": 610
    },
    {
      "epoch": 0.3668096435438545,
      "grad_norm": 0.1652083694934845,
      "learning_rate": 0.0001854287403903016,
      "loss": 0.1771,
      "step": 620
    },
    {
      "epoch": 0.37272592811714245,
      "grad_norm": 0.21508535742759705,
      "learning_rate": 0.00018519219396806623,
      "loss": 0.1983,
      "step": 630
    },
    {
      "epoch": 0.3786422126904304,
      "grad_norm": 0.14458397030830383,
      "learning_rate": 0.0001849556475458309,
      "loss": 0.2056,
      "step": 640
    },
    {
      "epoch": 0.3845584972637184,
      "grad_norm": 0.1806219071149826,
      "learning_rate": 0.00018471910112359552,
      "loss": 0.1976,
      "step": 650
    },
    {
      "epoch": 0.39047478183700635,
      "grad_norm": 0.2097308188676834,
      "learning_rate": 0.00018448255470136015,
      "loss": 0.2105,
      "step": 660
    },
    {
      "epoch": 0.3963910664102943,
      "grad_norm": 0.17379778623580933,
      "learning_rate": 0.0001842460082791248,
      "loss": 0.1917,
      "step": 670
    },
    {
      "epoch": 0.40230735098358233,
      "grad_norm": 0.1756100207567215,
      "learning_rate": 0.00018400946185688944,
      "loss": 0.1968,
      "step": 680
    },
    {
      "epoch": 0.4082236355568703,
      "grad_norm": 0.2504425644874573,
      "learning_rate": 0.00018377291543465407,
      "loss": 0.1853,
      "step": 690
    },
    {
      "epoch": 0.41413992013015827,
      "grad_norm": 0.19367481768131256,
      "learning_rate": 0.0001835363690124187,
      "loss": 0.1852,
      "step": 700
    },
    {
      "epoch": 0.42005620470344623,
      "grad_norm": 0.17047569155693054,
      "learning_rate": 0.00018329982259018333,
      "loss": 0.195,
      "step": 710
    },
    {
      "epoch": 0.4259724892767342,
      "grad_norm": 0.16140013933181763,
      "learning_rate": 0.00018306327616794796,
      "loss": 0.1964,
      "step": 720
    },
    {
      "epoch": 0.43188877385002217,
      "grad_norm": 0.1520240604877472,
      "learning_rate": 0.0001828267297457126,
      "loss": 0.1968,
      "step": 730
    },
    {
      "epoch": 0.43780505842331013,
      "grad_norm": 0.17908428609371185,
      "learning_rate": 0.00018259018332347725,
      "loss": 0.1906,
      "step": 740
    },
    {
      "epoch": 0.44372134299659816,
      "grad_norm": 0.18955031037330627,
      "learning_rate": 0.00018235363690124188,
      "loss": 0.1817,
      "step": 750
    },
    {
      "epoch": 0.4496376275698861,
      "grad_norm": 0.18578749895095825,
      "learning_rate": 0.0001821170904790065,
      "loss": 0.1921,
      "step": 760
    },
    {
      "epoch": 0.4555539121431741,
      "grad_norm": 0.18644848465919495,
      "learning_rate": 0.00018188054405677117,
      "loss": 0.2144,
      "step": 770
    },
    {
      "epoch": 0.46147019671646206,
      "grad_norm": 0.1910666972398758,
      "learning_rate": 0.0001816439976345358,
      "loss": 0.1972,
      "step": 780
    },
    {
      "epoch": 0.46738648128975,
      "grad_norm": 0.1939828097820282,
      "learning_rate": 0.00018140745121230043,
      "loss": 0.2017,
      "step": 790
    },
    {
      "epoch": 0.473302765863038,
      "grad_norm": 0.144367516040802,
      "learning_rate": 0.00018117090479006506,
      "loss": 0.1904,
      "step": 800
    },
    {
      "epoch": 0.479219050436326,
      "grad_norm": 0.1600876897573471,
      "learning_rate": 0.00018093435836782971,
      "loss": 0.1943,
      "step": 810
    },
    {
      "epoch": 0.485135335009614,
      "grad_norm": 0.17320087552070618,
      "learning_rate": 0.00018069781194559434,
      "loss": 0.2163,
      "step": 820
    },
    {
      "epoch": 0.49105161958290194,
      "grad_norm": 0.15391719341278076,
      "learning_rate": 0.00018046126552335895,
      "loss": 0.198,
      "step": 830
    },
    {
      "epoch": 0.4969679041561899,
      "grad_norm": 0.17956432700157166,
      "learning_rate": 0.0001802247191011236,
      "loss": 0.1958,
      "step": 840
    },
    {
      "epoch": 0.5028841887294779,
      "grad_norm": 0.18016333878040314,
      "learning_rate": 0.00017998817267888824,
      "loss": 0.1895,
      "step": 850
    },
    {
      "epoch": 0.5088004733027659,
      "grad_norm": 0.2130071520805359,
      "learning_rate": 0.00017975162625665287,
      "loss": 0.2097,
      "step": 860
    },
    {
      "epoch": 0.5147167578760539,
      "grad_norm": 0.12917102873325348,
      "learning_rate": 0.0001795150798344175,
      "loss": 0.1962,
      "step": 870
    },
    {
      "epoch": 0.5206330424493418,
      "grad_norm": 0.1569223552942276,
      "learning_rate": 0.00017927853341218215,
      "loss": 0.1932,
      "step": 880
    },
    {
      "epoch": 0.5265493270226298,
      "grad_norm": 0.16877959668636322,
      "learning_rate": 0.00017904198698994679,
      "loss": 0.1978,
      "step": 890
    },
    {
      "epoch": 0.5324656115959178,
      "grad_norm": 0.17393535375595093,
      "learning_rate": 0.00017880544056771142,
      "loss": 0.2158,
      "step": 900
    },
    {
      "epoch": 0.5383818961692057,
      "grad_norm": 0.15804846584796906,
      "learning_rate": 0.00017856889414547607,
      "loss": 0.2004,
      "step": 910
    },
    {
      "epoch": 0.5442981807424937,
      "grad_norm": 0.17609089612960815,
      "learning_rate": 0.0001783323477232407,
      "loss": 0.1978,
      "step": 920
    },
    {
      "epoch": 0.5502144653157817,
      "grad_norm": 0.16683267056941986,
      "learning_rate": 0.00017809580130100533,
      "loss": 0.1993,
      "step": 930
    },
    {
      "epoch": 0.5561307498890696,
      "grad_norm": 0.17970354855060577,
      "learning_rate": 0.00017785925487876996,
      "loss": 0.2215,
      "step": 940
    },
    {
      "epoch": 0.5620470344623576,
      "grad_norm": 0.182341530919075,
      "learning_rate": 0.00017762270845653462,
      "loss": 0.1806,
      "step": 950
    },
    {
      "epoch": 0.5679633190356456,
      "grad_norm": 0.15434414148330688,
      "learning_rate": 0.00017738616203429923,
      "loss": 0.1967,
      "step": 960
    },
    {
      "epoch": 0.5738796036089336,
      "grad_norm": 0.1556352972984314,
      "learning_rate": 0.00017714961561206386,
      "loss": 0.2136,
      "step": 970
    },
    {
      "epoch": 0.5797958881822216,
      "grad_norm": 0.16078102588653564,
      "learning_rate": 0.00017691306918982851,
      "loss": 0.1937,
      "step": 980
    },
    {
      "epoch": 0.5857121727555096,
      "grad_norm": 0.1428191363811493,
      "learning_rate": 0.00017667652276759314,
      "loss": 0.1869,
      "step": 990
    },
    {
      "epoch": 0.5916284573287975,
      "grad_norm": 0.14775237441062927,
      "learning_rate": 0.00017643997634535777,
      "loss": 0.1991,
      "step": 1000
    },
    {
      "epoch": 0.5975447419020855,
      "grad_norm": 0.19415827095508575,
      "learning_rate": 0.00017620342992312243,
      "loss": 0.1983,
      "step": 1010
    },
    {
      "epoch": 0.6034610264753735,
      "grad_norm": 0.18799932301044464,
      "learning_rate": 0.00017596688350088706,
      "loss": 0.1824,
      "step": 1020
    },
    {
      "epoch": 0.6093773110486614,
      "grad_norm": 0.17301085591316223,
      "learning_rate": 0.0001757303370786517,
      "loss": 0.1883,
      "step": 1030
    },
    {
      "epoch": 0.6152935956219494,
      "grad_norm": 0.1465160846710205,
      "learning_rate": 0.00017549379065641632,
      "loss": 0.1809,
      "step": 1040
    },
    {
      "epoch": 0.6212098801952374,
      "grad_norm": 0.19399048388004303,
      "learning_rate": 0.00017525724423418098,
      "loss": 0.1803,
      "step": 1050
    },
    {
      "epoch": 0.6271261647685253,
      "grad_norm": 0.16336356103420258,
      "learning_rate": 0.0001750206978119456,
      "loss": 0.2009,
      "step": 1060
    },
    {
      "epoch": 0.6330424493418133,
      "grad_norm": 0.1698412001132965,
      "learning_rate": 0.00017478415138971024,
      "loss": 0.1974,
      "step": 1070
    },
    {
      "epoch": 0.6389587339151013,
      "grad_norm": 0.1759575754404068,
      "learning_rate": 0.00017454760496747487,
      "loss": 0.1925,
      "step": 1080
    },
    {
      "epoch": 0.6448750184883892,
      "grad_norm": 0.1640731692314148,
      "learning_rate": 0.0001743110585452395,
      "loss": 0.1839,
      "step": 1090
    },
    {
      "epoch": 0.6507913030616773,
      "grad_norm": 0.1718469262123108,
      "learning_rate": 0.00017407451212300413,
      "loss": 0.1964,
      "step": 1100
    },
    {
      "epoch": 0.6567075876349653,
      "grad_norm": 0.13458149135112762,
      "learning_rate": 0.0001738379657007688,
      "loss": 0.2179,
      "step": 1110
    },
    {
      "epoch": 0.6626238722082533,
      "grad_norm": 0.14976072311401367,
      "learning_rate": 0.00017360141927853342,
      "loss": 0.2035,
      "step": 1120
    },
    {
      "epoch": 0.6685401567815412,
      "grad_norm": 0.1553380787372589,
      "learning_rate": 0.00017336487285629805,
      "loss": 0.1821,
      "step": 1130
    },
    {
      "epoch": 0.6744564413548292,
      "grad_norm": 0.1261233240365982,
      "learning_rate": 0.00017312832643406268,
      "loss": 0.1931,
      "step": 1140
    },
    {
      "epoch": 0.6803727259281172,
      "grad_norm": 0.2135489135980606,
      "learning_rate": 0.00017289178001182734,
      "loss": 0.205,
      "step": 1150
    },
    {
      "epoch": 0.6862890105014051,
      "grad_norm": 0.20174984633922577,
      "learning_rate": 0.00017265523358959197,
      "loss": 0.1787,
      "step": 1160
    },
    {
      "epoch": 0.6922052950746931,
      "grad_norm": 0.12781022489070892,
      "learning_rate": 0.0001724186871673566,
      "loss": 0.1876,
      "step": 1170
    },
    {
      "epoch": 0.698121579647981,
      "grad_norm": 0.15996509790420532,
      "learning_rate": 0.00017218214074512126,
      "loss": 0.1874,
      "step": 1180
    },
    {
      "epoch": 0.704037864221269,
      "grad_norm": 0.12115196883678436,
      "learning_rate": 0.0001719455943228859,
      "loss": 0.1885,
      "step": 1190
    },
    {
      "epoch": 0.709954148794557,
      "grad_norm": 0.19223180413246155,
      "learning_rate": 0.00017170904790065052,
      "loss": 0.1782,
      "step": 1200
    },
    {
      "epoch": 0.715870433367845,
      "grad_norm": 0.16311568021774292,
      "learning_rate": 0.00017147250147841515,
      "loss": 0.2017,
      "step": 1210
    },
    {
      "epoch": 0.7217867179411329,
      "grad_norm": 0.15063242614269257,
      "learning_rate": 0.00017123595505617978,
      "loss": 0.2131,
      "step": 1220
    },
    {
      "epoch": 0.727703002514421,
      "grad_norm": 0.13306188583374023,
      "learning_rate": 0.0001709994086339444,
      "loss": 0.1928,
      "step": 1230
    },
    {
      "epoch": 0.733619287087709,
      "grad_norm": 0.1508437991142273,
      "learning_rate": 0.00017076286221170904,
      "loss": 0.1861,
      "step": 1240
    },
    {
      "epoch": 0.7395355716609969,
      "grad_norm": 0.1457441747188568,
      "learning_rate": 0.0001705263157894737,
      "loss": 0.2135,
      "step": 1250
    },
    {
      "epoch": 0.7454518562342849,
      "grad_norm": 0.13277125358581543,
      "learning_rate": 0.00017028976936723833,
      "loss": 0.1827,
      "step": 1260
    },
    {
      "epoch": 0.7513681408075729,
      "grad_norm": 0.1631208211183548,
      "learning_rate": 0.00017005322294500296,
      "loss": 0.1961,
      "step": 1270
    },
    {
      "epoch": 0.7572844253808608,
      "grad_norm": 0.1696271151304245,
      "learning_rate": 0.0001698166765227676,
      "loss": 0.2077,
      "step": 1280
    },
    {
      "epoch": 0.7632007099541488,
      "grad_norm": 0.1712532490491867,
      "learning_rate": 0.00016958013010053225,
      "loss": 0.2044,
      "step": 1290
    },
    {
      "epoch": 0.7691169945274368,
      "grad_norm": 0.1588830053806305,
      "learning_rate": 0.00016934358367829688,
      "loss": 0.1896,
      "step": 1300
    },
    {
      "epoch": 0.7750332791007247,
      "grad_norm": 0.1528697907924652,
      "learning_rate": 0.0001691070372560615,
      "loss": 0.1788,
      "step": 1310
    },
    {
      "epoch": 0.7809495636740127,
      "grad_norm": 0.17082616686820984,
      "learning_rate": 0.00016887049083382617,
      "loss": 0.1965,
      "step": 1320
    },
    {
      "epoch": 0.7868658482473007,
      "grad_norm": 0.1482509970664978,
      "learning_rate": 0.00016863394441159077,
      "loss": 0.1921,
      "step": 1330
    },
    {
      "epoch": 0.7927821328205886,
      "grad_norm": 0.1788581907749176,
      "learning_rate": 0.0001683973979893554,
      "loss": 0.1798,
      "step": 1340
    },
    {
      "epoch": 0.7986984173938766,
      "grad_norm": 0.13707613945007324,
      "learning_rate": 0.00016816085156712006,
      "loss": 0.1815,
      "step": 1350
    },
    {
      "epoch": 0.8046147019671647,
      "grad_norm": 0.13468778133392334,
      "learning_rate": 0.0001679243051448847,
      "loss": 0.1855,
      "step": 1360
    },
    {
      "epoch": 0.8105309865404526,
      "grad_norm": 0.14956821501255035,
      "learning_rate": 0.00016768775872264932,
      "loss": 0.1918,
      "step": 1370
    },
    {
      "epoch": 0.8164472711137406,
      "grad_norm": 0.2048090249300003,
      "learning_rate": 0.00016745121230041395,
      "loss": 0.1849,
      "step": 1380
    },
    {
      "epoch": 0.8223635556870286,
      "grad_norm": 0.14784179627895355,
      "learning_rate": 0.0001672146658781786,
      "loss": 0.1799,
      "step": 1390
    },
    {
      "epoch": 0.8282798402603165,
      "grad_norm": 0.14125856757164001,
      "learning_rate": 0.00016697811945594324,
      "loss": 0.2025,
      "step": 1400
    },
    {
      "epoch": 0.8341961248336045,
      "grad_norm": 0.16123342514038086,
      "learning_rate": 0.00016674157303370787,
      "loss": 0.1787,
      "step": 1410
    },
    {
      "epoch": 0.8401124094068925,
      "grad_norm": 0.1875714808702469,
      "learning_rate": 0.00016650502661147253,
      "loss": 0.1832,
      "step": 1420
    },
    {
      "epoch": 0.8460286939801804,
      "grad_norm": 0.1751883178949356,
      "learning_rate": 0.00016626848018923716,
      "loss": 0.1833,
      "step": 1430
    },
    {
      "epoch": 0.8519449785534684,
      "grad_norm": 0.1651180237531662,
      "learning_rate": 0.0001660319337670018,
      "loss": 0.1959,
      "step": 1440
    },
    {
      "epoch": 0.8578612631267564,
      "grad_norm": 0.15895095467567444,
      "learning_rate": 0.00016579538734476642,
      "loss": 0.1821,
      "step": 1450
    },
    {
      "epoch": 0.8637775477000443,
      "grad_norm": 0.1741216778755188,
      "learning_rate": 0.00016555884092253105,
      "loss": 0.1902,
      "step": 1460
    },
    {
      "epoch": 0.8696938322733323,
      "grad_norm": 0.1685202419757843,
      "learning_rate": 0.00016532229450029568,
      "loss": 0.198,
      "step": 1470
    },
    {
      "epoch": 0.8756101168466203,
      "grad_norm": 0.15858381986618042,
      "learning_rate": 0.0001650857480780603,
      "loss": 0.1769,
      "step": 1480
    },
    {
      "epoch": 0.8815264014199083,
      "grad_norm": 0.15954072773456573,
      "learning_rate": 0.00016484920165582497,
      "loss": 0.1914,
      "step": 1490
    },
    {
      "epoch": 0.8874426859931963,
      "grad_norm": 0.17239978909492493,
      "learning_rate": 0.0001646126552335896,
      "loss": 0.1849,
      "step": 1500
    },
    {
      "epoch": 0.8933589705664843,
      "grad_norm": 0.15355807542800903,
      "learning_rate": 0.00016437610881135423,
      "loss": 0.1892,
      "step": 1510
    },
    {
      "epoch": 0.8992752551397722,
      "grad_norm": 0.18200626969337463,
      "learning_rate": 0.00016413956238911888,
      "loss": 0.1902,
      "step": 1520
    },
    {
      "epoch": 0.9051915397130602,
      "grad_norm": 0.1934819221496582,
      "learning_rate": 0.00016390301596688352,
      "loss": 0.1772,
      "step": 1530
    },
    {
      "epoch": 0.9111078242863482,
      "grad_norm": 0.1656295210123062,
      "learning_rate": 0.00016366646954464815,
      "loss": 0.1952,
      "step": 1540
    },
    {
      "epoch": 0.9170241088596361,
      "grad_norm": 0.21508929133415222,
      "learning_rate": 0.00016342992312241278,
      "loss": 0.1915,
      "step": 1550
    },
    {
      "epoch": 0.9229403934329241,
      "grad_norm": 0.1547517478466034,
      "learning_rate": 0.00016319337670017743,
      "loss": 0.1725,
      "step": 1560
    },
    {
      "epoch": 0.9288566780062121,
      "grad_norm": 0.17056159675121307,
      "learning_rate": 0.00016295683027794206,
      "loss": 0.1942,
      "step": 1570
    },
    {
      "epoch": 0.9347729625795,
      "grad_norm": 0.14956039190292358,
      "learning_rate": 0.0001627202838557067,
      "loss": 0.189,
      "step": 1580
    },
    {
      "epoch": 0.940689247152788,
      "grad_norm": 0.138031467795372,
      "learning_rate": 0.00016248373743347133,
      "loss": 0.1856,
      "step": 1590
    },
    {
      "epoch": 0.946605531726076,
      "grad_norm": 0.16677159070968628,
      "learning_rate": 0.00016224719101123596,
      "loss": 0.1902,
      "step": 1600
    },
    {
      "epoch": 0.952521816299364,
      "grad_norm": 0.1630767583847046,
      "learning_rate": 0.00016201064458900059,
      "loss": 0.1815,
      "step": 1610
    },
    {
      "epoch": 0.958438100872652,
      "grad_norm": 0.16919900476932526,
      "learning_rate": 0.00016177409816676524,
      "loss": 0.2022,
      "step": 1620
    },
    {
      "epoch": 0.96435438544594,
      "grad_norm": 0.1618266999721527,
      "learning_rate": 0.00016153755174452987,
      "loss": 0.1946,
      "step": 1630
    },
    {
      "epoch": 0.970270670019228,
      "grad_norm": 0.1707582026720047,
      "learning_rate": 0.0001613010053222945,
      "loss": 0.1841,
      "step": 1640
    },
    {
      "epoch": 0.9761869545925159,
      "grad_norm": 0.16140194237232208,
      "learning_rate": 0.00016106445890005914,
      "loss": 0.1822,
      "step": 1650
    },
    {
      "epoch": 0.9821032391658039,
      "grad_norm": 0.1402263641357422,
      "learning_rate": 0.0001608279124778238,
      "loss": 0.1859,
      "step": 1660
    },
    {
      "epoch": 0.9880195237390919,
      "grad_norm": 0.15657120943069458,
      "learning_rate": 0.00016059136605558842,
      "loss": 0.1883,
      "step": 1670
    },
    {
      "epoch": 0.9939358083123798,
      "grad_norm": 0.14896059036254883,
      "learning_rate": 0.00016035481963335305,
      "loss": 0.1697,
      "step": 1680
    },
    {
      "epoch": 0.9998520928856678,
      "grad_norm": 0.14497080445289612,
      "learning_rate": 0.00016011827321111768,
      "loss": 0.2062,
      "step": 1690
    },
    {
      "epoch": 1.0053246561159592,
      "grad_norm": 0.1638951301574707,
      "learning_rate": 0.00015988172678888234,
      "loss": 0.1776,
      "step": 1700
    },
    {
      "epoch": 1.011240940689247,
      "grad_norm": 0.1780993491411209,
      "learning_rate": 0.00015964518036664695,
      "loss": 0.1706,
      "step": 1710
    },
    {
      "epoch": 1.0171572252625352,
      "grad_norm": 0.16121900081634521,
      "learning_rate": 0.0001594086339444116,
      "loss": 0.1917,
      "step": 1720
    },
    {
      "epoch": 1.023073509835823,
      "grad_norm": 0.1536743938922882,
      "learning_rate": 0.00015917208752217623,
      "loss": 0.1666,
      "step": 1730
    },
    {
      "epoch": 1.028989794409111,
      "grad_norm": 0.14147448539733887,
      "learning_rate": 0.00015893554109994086,
      "loss": 0.1841,
      "step": 1740
    },
    {
      "epoch": 1.034906078982399,
      "grad_norm": 0.20811112225055695,
      "learning_rate": 0.0001586989946777055,
      "loss": 0.2003,
      "step": 1750
    },
    {
      "epoch": 1.040822363555687,
      "grad_norm": 0.16331931948661804,
      "learning_rate": 0.00015846244825547015,
      "loss": 0.1872,
      "step": 1760
    },
    {
      "epoch": 1.046738648128975,
      "grad_norm": 0.21611402928829193,
      "learning_rate": 0.00015822590183323478,
      "loss": 0.1788,
      "step": 1770
    },
    {
      "epoch": 1.052654932702263,
      "grad_norm": 0.16891925036907196,
      "learning_rate": 0.0001579893554109994,
      "loss": 0.1776,
      "step": 1780
    },
    {
      "epoch": 1.058571217275551,
      "grad_norm": 0.16817328333854675,
      "learning_rate": 0.00015775280898876404,
      "loss": 0.1864,
      "step": 1790
    },
    {
      "epoch": 1.064487501848839,
      "grad_norm": 0.1911301463842392,
      "learning_rate": 0.0001575162625665287,
      "loss": 0.1822,
      "step": 1800
    },
    {
      "epoch": 1.070403786422127,
      "grad_norm": 0.14819516241550446,
      "learning_rate": 0.00015727971614429333,
      "loss": 0.1796,
      "step": 1810
    },
    {
      "epoch": 1.0763200709954148,
      "grad_norm": 0.1781165450811386,
      "learning_rate": 0.00015704316972205796,
      "loss": 0.1925,
      "step": 1820
    },
    {
      "epoch": 1.082236355568703,
      "grad_norm": 0.19327038526535034,
      "learning_rate": 0.00015680662329982262,
      "loss": 0.1796,
      "step": 1830
    },
    {
      "epoch": 1.0881526401419908,
      "grad_norm": 0.1469888687133789,
      "learning_rate": 0.00015657007687758722,
      "loss": 0.1772,
      "step": 1840
    },
    {
      "epoch": 1.0940689247152788,
      "grad_norm": 0.1863192766904831,
      "learning_rate": 0.00015633353045535185,
      "loss": 0.2031,
      "step": 1850
    },
    {
      "epoch": 1.0999852092885667,
      "grad_norm": 0.17902661859989166,
      "learning_rate": 0.0001560969840331165,
      "loss": 0.1745,
      "step": 1860
    },
    {
      "epoch": 1.1059014938618548,
      "grad_norm": 0.15134641528129578,
      "learning_rate": 0.00015586043761088114,
      "loss": 0.1702,
      "step": 1870
    },
    {
      "epoch": 1.1118177784351428,
      "grad_norm": 0.1463027447462082,
      "learning_rate": 0.00015562389118864577,
      "loss": 0.1791,
      "step": 1880
    },
    {
      "epoch": 1.1177340630084307,
      "grad_norm": 0.1792365163564682,
      "learning_rate": 0.0001553873447664104,
      "loss": 0.1735,
      "step": 1890
    },
    {
      "epoch": 1.1236503475817188,
      "grad_norm": 0.1715850830078125,
      "learning_rate": 0.00015515079834417506,
      "loss": 0.1794,
      "step": 1900
    },
    {
      "epoch": 1.1295666321550066,
      "grad_norm": 0.16203057765960693,
      "learning_rate": 0.0001549142519219397,
      "loss": 0.1788,
      "step": 1910
    },
    {
      "epoch": 1.1354829167282947,
      "grad_norm": 0.18869565427303314,
      "learning_rate": 0.00015467770549970432,
      "loss": 0.1867,
      "step": 1920
    },
    {
      "epoch": 1.1413992013015826,
      "grad_norm": 0.14225779473781586,
      "learning_rate": 0.00015444115907746898,
      "loss": 0.1728,
      "step": 1930
    },
    {
      "epoch": 1.1473154858748706,
      "grad_norm": 0.1794026792049408,
      "learning_rate": 0.0001542046126552336,
      "loss": 0.2021,
      "step": 1940
    },
    {
      "epoch": 1.1532317704481585,
      "grad_norm": 0.15789468586444855,
      "learning_rate": 0.00015396806623299824,
      "loss": 0.1737,
      "step": 1950
    },
    {
      "epoch": 1.1591480550214466,
      "grad_norm": 0.17932303249835968,
      "learning_rate": 0.00015373151981076287,
      "loss": 0.1914,
      "step": 1960
    },
    {
      "epoch": 1.1650643395947344,
      "grad_norm": 0.1814931333065033,
      "learning_rate": 0.0001534949733885275,
      "loss": 0.1737,
      "step": 1970
    },
    {
      "epoch": 1.1709806241680225,
      "grad_norm": 0.15568576753139496,
      "learning_rate": 0.00015325842696629213,
      "loss": 0.19,
      "step": 1980
    },
    {
      "epoch": 1.1768969087413104,
      "grad_norm": 0.17178764939308167,
      "learning_rate": 0.00015302188054405676,
      "loss": 0.1778,
      "step": 1990
    },
    {
      "epoch": 1.1828131933145984,
      "grad_norm": 0.18548712134361267,
      "learning_rate": 0.00015278533412182142,
      "loss": 0.1714,
      "step": 2000
    },
    {
      "epoch": 1.1887294778878865,
      "grad_norm": 0.16588738560676575,
      "learning_rate": 0.00015254878769958605,
      "loss": 0.1734,
      "step": 2010
    },
    {
      "epoch": 1.1946457624611744,
      "grad_norm": 0.16310378909111023,
      "learning_rate": 0.00015231224127735068,
      "loss": 0.1914,
      "step": 2020
    },
    {
      "epoch": 1.2005620470344625,
      "grad_norm": 0.17498187720775604,
      "learning_rate": 0.00015207569485511534,
      "loss": 0.1804,
      "step": 2030
    },
    {
      "epoch": 1.2064783316077503,
      "grad_norm": 0.17078839242458344,
      "learning_rate": 0.00015183914843287997,
      "loss": 0.1661,
      "step": 2040
    },
    {
      "epoch": 1.2123946161810384,
      "grad_norm": 0.17553620040416718,
      "learning_rate": 0.0001516026020106446,
      "loss": 0.1822,
      "step": 2050
    },
    {
      "epoch": 1.2183109007543262,
      "grad_norm": 0.16861246526241302,
      "learning_rate": 0.00015136605558840923,
      "loss": 0.1785,
      "step": 2060
    },
    {
      "epoch": 1.2242271853276143,
      "grad_norm": 0.1635797917842865,
      "learning_rate": 0.0001511295091661739,
      "loss": 0.1691,
      "step": 2070
    },
    {
      "epoch": 1.2301434699009022,
      "grad_norm": 0.19050776958465576,
      "learning_rate": 0.00015089296274393852,
      "loss": 0.1806,
      "step": 2080
    },
    {
      "epoch": 1.2360597544741903,
      "grad_norm": 0.13387033343315125,
      "learning_rate": 0.00015065641632170312,
      "loss": 0.1751,
      "step": 2090
    },
    {
      "epoch": 1.241976039047478,
      "grad_norm": 0.1372293382883072,
      "learning_rate": 0.00015041986989946778,
      "loss": 0.1658,
      "step": 2100
    },
    {
      "epoch": 1.2478923236207662,
      "grad_norm": 0.1824548840522766,
      "learning_rate": 0.0001501833234772324,
      "loss": 0.1919,
      "step": 2110
    },
    {
      "epoch": 1.253808608194054,
      "grad_norm": 0.16336804628372192,
      "learning_rate": 0.00014994677705499704,
      "loss": 0.1873,
      "step": 2120
    },
    {
      "epoch": 1.2597248927673421,
      "grad_norm": 0.15059040486812592,
      "learning_rate": 0.0001497102306327617,
      "loss": 0.1824,
      "step": 2130
    },
    {
      "epoch": 1.2656411773406302,
      "grad_norm": 0.20250768959522247,
      "learning_rate": 0.00014947368421052633,
      "loss": 0.1934,
      "step": 2140
    },
    {
      "epoch": 1.271557461913918,
      "grad_norm": 0.16847312450408936,
      "learning_rate": 0.00014923713778829096,
      "loss": 0.1785,
      "step": 2150
    },
    {
      "epoch": 1.277473746487206,
      "grad_norm": 0.16946864128112793,
      "learning_rate": 0.0001490005913660556,
      "loss": 0.1791,
      "step": 2160
    },
    {
      "epoch": 1.283390031060494,
      "grad_norm": 0.15537972748279572,
      "learning_rate": 0.00014876404494382025,
      "loss": 0.1819,
      "step": 2170
    },
    {
      "epoch": 1.289306315633782,
      "grad_norm": 0.18462611734867096,
      "learning_rate": 0.00014852749852158488,
      "loss": 0.1759,
      "step": 2180
    },
    {
      "epoch": 1.29522260020707,
      "grad_norm": 0.15257714688777924,
      "learning_rate": 0.0001482909520993495,
      "loss": 0.1731,
      "step": 2190
    },
    {
      "epoch": 1.301138884780358,
      "grad_norm": 0.1582348644733429,
      "learning_rate": 0.00014805440567711414,
      "loss": 0.1866,
      "step": 2200
    },
    {
      "epoch": 1.3070551693536459,
      "grad_norm": 0.17086951434612274,
      "learning_rate": 0.0001478178592548788,
      "loss": 0.1673,
      "step": 2210
    },
    {
      "epoch": 1.312971453926934,
      "grad_norm": 0.1675882786512375,
      "learning_rate": 0.0001475813128326434,
      "loss": 0.1932,
      "step": 2220
    },
    {
      "epoch": 1.3188877385002218,
      "grad_norm": 0.15892253816127777,
      "learning_rate": 0.00014734476641040803,
      "loss": 0.1662,
      "step": 2230
    },
    {
      "epoch": 1.3248040230735099,
      "grad_norm": 0.15201254189014435,
      "learning_rate": 0.00014710821998817269,
      "loss": 0.1753,
      "step": 2240
    },
    {
      "epoch": 1.3307203076467977,
      "grad_norm": 0.16783356666564941,
      "learning_rate": 0.00014687167356593732,
      "loss": 0.1791,
      "step": 2250
    },
    {
      "epoch": 1.3366365922200858,
      "grad_norm": 0.1751270741224289,
      "learning_rate": 0.00014663512714370195,
      "loss": 0.173,
      "step": 2260
    },
    {
      "epoch": 1.3425528767933739,
      "grad_norm": 0.16416573524475098,
      "learning_rate": 0.0001463985807214666,
      "loss": 0.1779,
      "step": 2270
    },
    {
      "epoch": 1.3484691613666617,
      "grad_norm": 0.21125201880931854,
      "learning_rate": 0.00014616203429923123,
      "loss": 0.191,
      "step": 2280
    },
    {
      "epoch": 1.3543854459399496,
      "grad_norm": 0.2150404453277588,
      "learning_rate": 0.00014592548787699587,
      "loss": 0.1704,
      "step": 2290
    },
    {
      "epoch": 1.3603017305132377,
      "grad_norm": 0.17723314464092255,
      "learning_rate": 0.0001456889414547605,
      "loss": 0.1914,
      "step": 2300
    },
    {
      "epoch": 1.3662180150865257,
      "grad_norm": 0.16549059748649597,
      "learning_rate": 0.00014545239503252515,
      "loss": 0.1809,
      "step": 2310
    },
    {
      "epoch": 1.3721342996598136,
      "grad_norm": 0.15161238610744476,
      "learning_rate": 0.00014521584861028978,
      "loss": 0.1803,
      "step": 2320
    },
    {
      "epoch": 1.3780505842331017,
      "grad_norm": 0.15584978461265564,
      "learning_rate": 0.00014497930218805441,
      "loss": 0.1736,
      "step": 2330
    },
    {
      "epoch": 1.3839668688063895,
      "grad_norm": 0.1758546680212021,
      "learning_rate": 0.00014474275576581904,
      "loss": 0.1773,
      "step": 2340
    },
    {
      "epoch": 1.3898831533796776,
      "grad_norm": 0.16584347188472748,
      "learning_rate": 0.00014450620934358368,
      "loss": 0.1752,
      "step": 2350
    },
    {
      "epoch": 1.3957994379529655,
      "grad_norm": 0.2130507528781891,
      "learning_rate": 0.0001442696629213483,
      "loss": 0.1847,
      "step": 2360
    },
    {
      "epoch": 1.4017157225262535,
      "grad_norm": 0.13960207998752594,
      "learning_rate": 0.00014403311649911296,
      "loss": 0.167,
      "step": 2370
    },
    {
      "epoch": 1.4076320070995414,
      "grad_norm": 0.17909333109855652,
      "learning_rate": 0.0001437965700768776,
      "loss": 0.1842,
      "step": 2380
    },
    {
      "epoch": 1.4135482916728295,
      "grad_norm": 0.20177772641181946,
      "learning_rate": 0.00014356002365464222,
      "loss": 0.2052,
      "step": 2390
    },
    {
      "epoch": 1.4194645762461175,
      "grad_norm": 0.1329110711812973,
      "learning_rate": 0.00014332347723240685,
      "loss": 0.1688,
      "step": 2400
    },
    {
      "epoch": 1.4253808608194054,
      "grad_norm": 0.17826874554157257,
      "learning_rate": 0.0001430869308101715,
      "loss": 0.1911,
      "step": 2410
    },
    {
      "epoch": 1.4312971453926933,
      "grad_norm": 0.18873262405395508,
      "learning_rate": 0.00014285038438793614,
      "loss": 0.1895,
      "step": 2420
    },
    {
      "epoch": 1.4372134299659813,
      "grad_norm": 0.18123909831047058,
      "learning_rate": 0.00014261383796570077,
      "loss": 0.1818,
      "step": 2430
    },
    {
      "epoch": 1.4431297145392694,
      "grad_norm": 0.1609157919883728,
      "learning_rate": 0.00014237729154346543,
      "loss": 0.173,
      "step": 2440
    },
    {
      "epoch": 1.4490459991125573,
      "grad_norm": 0.15844018757343292,
      "learning_rate": 0.00014214074512123006,
      "loss": 0.1784,
      "step": 2450
    },
    {
      "epoch": 1.4549622836858453,
      "grad_norm": 0.1857662945985794,
      "learning_rate": 0.0001419041986989947,
      "loss": 0.1863,
      "step": 2460
    },
    {
      "epoch": 1.4608785682591332,
      "grad_norm": 0.17263686656951904,
      "learning_rate": 0.00014166765227675932,
      "loss": 0.1853,
      "step": 2470
    },
    {
      "epoch": 1.4667948528324213,
      "grad_norm": 0.16169926524162292,
      "learning_rate": 0.00014143110585452395,
      "loss": 0.2008,
      "step": 2480
    },
    {
      "epoch": 1.4727111374057091,
      "grad_norm": 0.16224734485149384,
      "learning_rate": 0.00014119455943228858,
      "loss": 0.1719,
      "step": 2490
    },
    {
      "epoch": 1.4786274219789972,
      "grad_norm": 0.23546190559864044,
      "learning_rate": 0.00014095801301005321,
      "loss": 0.1757,
      "step": 2500
    },
    {
      "epoch": 1.484543706552285,
      "grad_norm": 0.17217279970645905,
      "learning_rate": 0.00014072146658781787,
      "loss": 0.1733,
      "step": 2510
    },
    {
      "epoch": 1.4904599911255731,
      "grad_norm": 0.18966661393642426,
      "learning_rate": 0.0001404849201655825,
      "loss": 0.1797,
      "step": 2520
    },
    {
      "epoch": 1.4963762756988612,
      "grad_norm": 0.15442590415477753,
      "learning_rate": 0.00014024837374334713,
      "loss": 0.1779,
      "step": 2530
    },
    {
      "epoch": 1.502292560272149,
      "grad_norm": 0.2111760377883911,
      "learning_rate": 0.0001400118273211118,
      "loss": 0.1775,
      "step": 2540
    },
    {
      "epoch": 1.508208844845437,
      "grad_norm": 0.17181900143623352,
      "learning_rate": 0.00013977528089887642,
      "loss": 0.1997,
      "step": 2550
    },
    {
      "epoch": 1.514125129418725,
      "grad_norm": 0.14365985989570618,
      "learning_rate": 0.00013953873447664105,
      "loss": 0.1746,
      "step": 2560
    },
    {
      "epoch": 1.520041413992013,
      "grad_norm": 0.20591115951538086,
      "learning_rate": 0.00013930218805440568,
      "loss": 0.1944,
      "step": 2570
    },
    {
      "epoch": 1.525957698565301,
      "grad_norm": 0.1982387900352478,
      "learning_rate": 0.00013906564163217034,
      "loss": 0.1783,
      "step": 2580
    },
    {
      "epoch": 1.5318739831385888,
      "grad_norm": 0.1984216570854187,
      "learning_rate": 0.00013882909520993497,
      "loss": 0.1896,
      "step": 2590
    },
    {
      "epoch": 1.5377902677118769,
      "grad_norm": 0.17610818147659302,
      "learning_rate": 0.00013859254878769957,
      "loss": 0.1769,
      "step": 2600
    },
    {
      "epoch": 1.543706552285165,
      "grad_norm": 0.17441412806510925,
      "learning_rate": 0.00013835600236546423,
      "loss": 0.1864,
      "step": 2610
    },
    {
      "epoch": 1.549622836858453,
      "grad_norm": 0.16621175408363342,
      "learning_rate": 0.00013811945594322886,
      "loss": 0.1702,
      "step": 2620
    },
    {
      "epoch": 1.555539121431741,
      "grad_norm": 0.1948402225971222,
      "learning_rate": 0.0001378829095209935,
      "loss": 0.1767,
      "step": 2630
    },
    {
      "epoch": 1.5614554060050287,
      "grad_norm": 0.19458195567131042,
      "learning_rate": 0.00013764636309875812,
      "loss": 0.187,
      "step": 2640
    },
    {
      "epoch": 1.5673716905783168,
      "grad_norm": 0.19468072056770325,
      "learning_rate": 0.00013740981667652278,
      "loss": 0.2079,
      "step": 2650
    },
    {
      "epoch": 1.573287975151605,
      "grad_norm": 0.16995592415332794,
      "learning_rate": 0.0001371732702542874,
      "loss": 0.1862,
      "step": 2660
    },
    {
      "epoch": 1.5792042597248928,
      "grad_norm": 0.2522299587726593,
      "learning_rate": 0.00013693672383205204,
      "loss": 0.1997,
      "step": 2670
    },
    {
      "epoch": 1.5851205442981806,
      "grad_norm": 0.185825377702713,
      "learning_rate": 0.0001367001774098167,
      "loss": 0.184,
      "step": 2680
    },
    {
      "epoch": 1.5910368288714687,
      "grad_norm": 0.16418269276618958,
      "learning_rate": 0.00013646363098758133,
      "loss": 0.1768,
      "step": 2690
    },
    {
      "epoch": 1.5969531134447568,
      "grad_norm": 0.21611841022968292,
      "learning_rate": 0.00013622708456534596,
      "loss": 0.1822,
      "step": 2700
    },
    {
      "epoch": 1.6028693980180446,
      "grad_norm": 0.16040180623531342,
      "learning_rate": 0.0001359905381431106,
      "loss": 0.1856,
      "step": 2710
    },
    {
      "epoch": 1.6087856825913325,
      "grad_norm": 0.16263318061828613,
      "learning_rate": 0.00013575399172087522,
      "loss": 0.171,
      "step": 2720
    },
    {
      "epoch": 1.6147019671646206,
      "grad_norm": 0.18594568967819214,
      "learning_rate": 0.00013551744529863985,
      "loss": 0.1885,
      "step": 2730
    },
    {
      "epoch": 1.6206182517379086,
      "grad_norm": 0.18630969524383545,
      "learning_rate": 0.00013528089887640448,
      "loss": 0.1779,
      "step": 2740
    },
    {
      "epoch": 1.6265345363111967,
      "grad_norm": 0.1928529143333435,
      "learning_rate": 0.00013504435245416914,
      "loss": 0.1833,
      "step": 2750
    },
    {
      "epoch": 1.6324508208844846,
      "grad_norm": 0.1577199250459671,
      "learning_rate": 0.00013480780603193377,
      "loss": 0.1746,
      "step": 2760
    },
    {
      "epoch": 1.6383671054577724,
      "grad_norm": 0.16398228704929352,
      "learning_rate": 0.0001345712596096984,
      "loss": 0.17,
      "step": 2770
    },
    {
      "epoch": 1.6442833900310605,
      "grad_norm": 0.17436519265174866,
      "learning_rate": 0.00013433471318746306,
      "loss": 0.1708,
      "step": 2780
    },
    {
      "epoch": 1.6501996746043486,
      "grad_norm": 0.2148803472518921,
      "learning_rate": 0.0001340981667652277,
      "loss": 0.1776,
      "step": 2790
    },
    {
      "epoch": 1.6561159591776364,
      "grad_norm": 0.18564148247241974,
      "learning_rate": 0.00013386162034299232,
      "loss": 0.1845,
      "step": 2800
    },
    {
      "epoch": 1.6620322437509243,
      "grad_norm": 0.17803798615932465,
      "learning_rate": 0.00013362507392075695,
      "loss": 0.1716,
      "step": 2810
    },
    {
      "epoch": 1.6679485283242124,
      "grad_norm": 0.23887625336647034,
      "learning_rate": 0.0001333885274985216,
      "loss": 0.1976,
      "step": 2820
    },
    {
      "epoch": 1.6738648128975004,
      "grad_norm": 0.19334524869918823,
      "learning_rate": 0.00013315198107628624,
      "loss": 0.1766,
      "step": 2830
    },
    {
      "epoch": 1.6797810974707883,
      "grad_norm": 0.15670591592788696,
      "learning_rate": 0.00013291543465405087,
      "loss": 0.1796,
      "step": 2840
    },
    {
      "epoch": 1.6856973820440762,
      "grad_norm": 0.15901774168014526,
      "learning_rate": 0.0001326788882318155,
      "loss": 0.1608,
      "step": 2850
    },
    {
      "epoch": 1.6916136666173642,
      "grad_norm": 0.194894477725029,
      "learning_rate": 0.00013244234180958013,
      "loss": 0.169,
      "step": 2860
    },
    {
      "epoch": 1.6975299511906523,
      "grad_norm": 0.13603462278842926,
      "learning_rate": 0.00013220579538734476,
      "loss": 0.1803,
      "step": 2870
    },
    {
      "epoch": 1.7034462357639404,
      "grad_norm": 0.18928149342536926,
      "learning_rate": 0.00013196924896510942,
      "loss": 0.1758,
      "step": 2880
    },
    {
      "epoch": 1.7093625203372282,
      "grad_norm": 0.18625934422016144,
      "learning_rate": 0.00013173270254287405,
      "loss": 0.1883,
      "step": 2890
    },
    {
      "epoch": 1.715278804910516,
      "grad_norm": 0.18171848356723785,
      "learning_rate": 0.00013149615612063868,
      "loss": 0.1703,
      "step": 2900
    },
    {
      "epoch": 1.7211950894838042,
      "grad_norm": 0.19508059322834015,
      "learning_rate": 0.0001312596096984033,
      "loss": 0.1707,
      "step": 2910
    },
    {
      "epoch": 1.7271113740570923,
      "grad_norm": 0.1766282320022583,
      "learning_rate": 0.00013102306327616797,
      "loss": 0.1935,
      "step": 2920
    },
    {
      "epoch": 1.73302765863038,
      "grad_norm": 0.1647951900959015,
      "learning_rate": 0.0001307865168539326,
      "loss": 0.1883,
      "step": 2930
    },
    {
      "epoch": 1.738943943203668,
      "grad_norm": 0.1559610366821289,
      "learning_rate": 0.00013054997043169723,
      "loss": 0.1832,
      "step": 2940
    },
    {
      "epoch": 1.744860227776956,
      "grad_norm": 0.1765676587820053,
      "learning_rate": 0.00013031342400946188,
      "loss": 0.186,
      "step": 2950
    },
    {
      "epoch": 1.7507765123502441,
      "grad_norm": 0.18346825242042542,
      "learning_rate": 0.00013007687758722651,
      "loss": 0.185,
      "step": 2960
    },
    {
      "epoch": 1.756692796923532,
      "grad_norm": 0.18079747259616852,
      "learning_rate": 0.00012984033116499112,
      "loss": 0.1904,
      "step": 2970
    },
    {
      "epoch": 1.76260908149682,
      "grad_norm": 0.19403794407844543,
      "learning_rate": 0.00012960378474275578,
      "loss": 0.1761,
      "step": 2980
    },
    {
      "epoch": 1.768525366070108,
      "grad_norm": 0.18266844749450684,
      "learning_rate": 0.0001293672383205204,
      "loss": 0.1834,
      "step": 2990
    },
    {
      "epoch": 1.774441650643396,
      "grad_norm": 0.22720938920974731,
      "learning_rate": 0.00012913069189828504,
      "loss": 0.1929,
      "step": 3000
    },
    {
      "epoch": 1.780357935216684,
      "grad_norm": 0.1915227621793747,
      "learning_rate": 0.00012889414547604967,
      "loss": 0.1877,
      "step": 3010
    },
    {
      "epoch": 1.786274219789972,
      "grad_norm": 0.14799386262893677,
      "learning_rate": 0.00012865759905381432,
      "loss": 0.1793,
      "step": 3020
    },
    {
      "epoch": 1.7921905043632598,
      "grad_norm": 0.1565966159105301,
      "learning_rate": 0.00012842105263157895,
      "loss": 0.1944,
      "step": 3030
    },
    {
      "epoch": 1.7981067889365479,
      "grad_norm": 0.1961866319179535,
      "learning_rate": 0.00012818450620934359,
      "loss": 0.1637,
      "step": 3040
    },
    {
      "epoch": 1.804023073509836,
      "grad_norm": 0.18865293264389038,
      "learning_rate": 0.00012794795978710822,
      "loss": 0.1782,
      "step": 3050
    },
    {
      "epoch": 1.8099393580831238,
      "grad_norm": 0.2210111767053604,
      "learning_rate": 0.00012771141336487287,
      "loss": 0.1856,
      "step": 3060
    },
    {
      "epoch": 1.8158556426564116,
      "grad_norm": 0.2227199524641037,
      "learning_rate": 0.0001274748669426375,
      "loss": 0.1807,
      "step": 3070
    },
    {
      "epoch": 1.8217719272296997,
      "grad_norm": 0.22785863280296326,
      "learning_rate": 0.00012723832052040213,
      "loss": 0.1696,
      "step": 3080
    },
    {
      "epoch": 1.8276882118029878,
      "grad_norm": 0.1785385012626648,
      "learning_rate": 0.0001270017740981668,
      "loss": 0.1859,
      "step": 3090
    },
    {
      "epoch": 1.8336044963762756,
      "grad_norm": 0.16264915466308594,
      "learning_rate": 0.0001267652276759314,
      "loss": 0.1746,
      "step": 3100
    },
    {
      "epoch": 1.8395207809495637,
      "grad_norm": 0.19420182704925537,
      "learning_rate": 0.00012652868125369603,
      "loss": 0.1951,
      "step": 3110
    },
    {
      "epoch": 1.8454370655228516,
      "grad_norm": 0.15906338393688202,
      "learning_rate": 0.00012629213483146068,
      "loss": 0.1761,
      "step": 3120
    },
    {
      "epoch": 1.8513533500961397,
      "grad_norm": 0.1997881978750229,
      "learning_rate": 0.00012605558840922531,
      "loss": 0.1875,
      "step": 3130
    },
    {
      "epoch": 1.8572696346694277,
      "grad_norm": 0.17630277574062347,
      "learning_rate": 0.00012581904198698994,
      "loss": 0.1773,
      "step": 3140
    },
    {
      "epoch": 1.8631859192427156,
      "grad_norm": 0.17853504419326782,
      "learning_rate": 0.00012558249556475457,
      "loss": 0.1725,
      "step": 3150
    },
    {
      "epoch": 1.8691022038160034,
      "grad_norm": 0.19728252291679382,
      "learning_rate": 0.00012534594914251923,
      "loss": 0.1792,
      "step": 3160
    },
    {
      "epoch": 1.8750184883892915,
      "grad_norm": 0.1709403395652771,
      "learning_rate": 0.00012510940272028386,
      "loss": 0.1766,
      "step": 3170
    },
    {
      "epoch": 1.8809347729625796,
      "grad_norm": 0.18436583876609802,
      "learning_rate": 0.0001248728562980485,
      "loss": 0.1785,
      "step": 3180
    },
    {
      "epoch": 1.8868510575358675,
      "grad_norm": 0.25344759225845337,
      "learning_rate": 0.00012463630987581315,
      "loss": 0.1812,
      "step": 3190
    },
    {
      "epoch": 1.8927673421091553,
      "grad_norm": 0.17291119694709778,
      "learning_rate": 0.00012439976345357778,
      "loss": 0.1902,
      "step": 3200
    },
    {
      "epoch": 1.8986836266824434,
      "grad_norm": 0.16246017813682556,
      "learning_rate": 0.0001241632170313424,
      "loss": 0.1671,
      "step": 3210
    },
    {
      "epoch": 1.9045999112557315,
      "grad_norm": 0.14193859696388245,
      "learning_rate": 0.00012392667060910704,
      "loss": 0.1866,
      "step": 3220
    },
    {
      "epoch": 1.9105161958290193,
      "grad_norm": 0.20581203699111938,
      "learning_rate": 0.00012369012418687167,
      "loss": 0.1898,
      "step": 3230
    },
    {
      "epoch": 1.9164324804023074,
      "grad_norm": 0.20807810127735138,
      "learning_rate": 0.0001234535777646363,
      "loss": 0.175,
      "step": 3240
    },
    {
      "epoch": 1.9223487649755953,
      "grad_norm": 0.20300798118114471,
      "learning_rate": 0.00012321703134240093,
      "loss": 0.179,
      "step": 3250
    },
    {
      "epoch": 1.9282650495488833,
      "grad_norm": 0.19917674362659454,
      "learning_rate": 0.0001229804849201656,
      "loss": 0.1904,
      "step": 3260
    },
    {
      "epoch": 1.9341813341221714,
      "grad_norm": 0.19484570622444153,
      "learning_rate": 0.00012274393849793022,
      "loss": 0.1918,
      "step": 3270
    },
    {
      "epoch": 1.9400976186954593,
      "grad_norm": 0.17390434443950653,
      "learning_rate": 0.00012250739207569485,
      "loss": 0.1777,
      "step": 3280
    },
    {
      "epoch": 1.9460139032687471,
      "grad_norm": 0.17685268819332123,
      "learning_rate": 0.0001222708456534595,
      "loss": 0.173,
      "step": 3290
    },
    {
      "epoch": 1.9519301878420352,
      "grad_norm": 0.18296115100383759,
      "learning_rate": 0.00012203429923122414,
      "loss": 0.1716,
      "step": 3300
    },
    {
      "epoch": 1.9578464724153233,
      "grad_norm": 0.15805567800998688,
      "learning_rate": 0.00012179775280898877,
      "loss": 0.1849,
      "step": 3310
    },
    {
      "epoch": 1.9637627569886111,
      "grad_norm": 0.25033852458000183,
      "learning_rate": 0.00012156120638675341,
      "loss": 0.1768,
      "step": 3320
    },
    {
      "epoch": 1.969679041561899,
      "grad_norm": 0.18556345999240875,
      "learning_rate": 0.00012132465996451805,
      "loss": 0.1705,
      "step": 3330
    },
    {
      "epoch": 1.975595326135187,
      "grad_norm": 0.16956955194473267,
      "learning_rate": 0.00012108811354228269,
      "loss": 0.1981,
      "step": 3340
    },
    {
      "epoch": 1.9815116107084751,
      "grad_norm": 0.17378713190555573,
      "learning_rate": 0.0001208515671200473,
      "loss": 0.183,
      "step": 3350
    },
    {
      "epoch": 1.987427895281763,
      "grad_norm": 0.1561620831489563,
      "learning_rate": 0.00012061502069781194,
      "loss": 0.1634,
      "step": 3360
    },
    {
      "epoch": 1.993344179855051,
      "grad_norm": 0.17841844260692596,
      "learning_rate": 0.00012037847427557658,
      "loss": 0.1715,
      "step": 3370
    },
    {
      "epoch": 1.999260464428339,
      "grad_norm": 0.1756368726491928,
      "learning_rate": 0.00012014192785334122,
      "loss": 0.1872,
      "step": 3380
    },
    {
      "epoch": 2.0047330276586304,
      "grad_norm": 0.1763206422328949,
      "learning_rate": 0.00011990538143110586,
      "loss": 0.1649,
      "step": 3390
    },
    {
      "epoch": 2.0106493122319185,
      "grad_norm": 0.21595145761966705,
      "learning_rate": 0.0001196688350088705,
      "loss": 0.1719,
      "step": 3400
    },
    {
      "epoch": 2.0165655968052065,
      "grad_norm": 0.2572387158870697,
      "learning_rate": 0.00011943228858663513,
      "loss": 0.1667,
      "step": 3410
    },
    {
      "epoch": 2.022481881378494,
      "grad_norm": 0.1895924210548401,
      "learning_rate": 0.00011919574216439977,
      "loss": 0.1766,
      "step": 3420
    },
    {
      "epoch": 2.0283981659517822,
      "grad_norm": 0.20613640546798706,
      "learning_rate": 0.0001189591957421644,
      "loss": 0.1762,
      "step": 3430
    },
    {
      "epoch": 2.0343144505250703,
      "grad_norm": 0.18584296107292175,
      "learning_rate": 0.00011872264931992905,
      "loss": 0.17,
      "step": 3440
    },
    {
      "epoch": 2.0402307350983584,
      "grad_norm": 0.18499940633773804,
      "learning_rate": 0.00011848610289769368,
      "loss": 0.1627,
      "step": 3450
    },
    {
      "epoch": 2.046147019671646,
      "grad_norm": 0.18388307094573975,
      "learning_rate": 0.00011824955647545832,
      "loss": 0.1826,
      "step": 3460
    },
    {
      "epoch": 2.052063304244934,
      "grad_norm": 0.16285653412342072,
      "learning_rate": 0.00011801301005322297,
      "loss": 0.1683,
      "step": 3470
    },
    {
      "epoch": 2.057979588818222,
      "grad_norm": 0.19476687908172607,
      "learning_rate": 0.00011777646363098757,
      "loss": 0.1503,
      "step": 3480
    },
    {
      "epoch": 2.0638958733915103,
      "grad_norm": 0.23604586720466614,
      "learning_rate": 0.00011753991720875221,
      "loss": 0.1804,
      "step": 3490
    },
    {
      "epoch": 2.069812157964798,
      "grad_norm": 0.18039998412132263,
      "learning_rate": 0.00011730337078651686,
      "loss": 0.1683,
      "step": 3500
    },
    {
      "epoch": 2.075728442538086,
      "grad_norm": 0.1668868362903595,
      "learning_rate": 0.00011706682436428149,
      "loss": 0.1562,
      "step": 3510
    },
    {
      "epoch": 2.081644727111374,
      "grad_norm": 0.19422270357608795,
      "learning_rate": 0.00011683027794204613,
      "loss": 0.1825,
      "step": 3520
    },
    {
      "epoch": 2.087561011684662,
      "grad_norm": 0.19017061591148376,
      "learning_rate": 0.00011659373151981076,
      "loss": 0.1696,
      "step": 3530
    },
    {
      "epoch": 2.09347729625795,
      "grad_norm": 0.16187037527561188,
      "learning_rate": 0.00011635718509757541,
      "loss": 0.1706,
      "step": 3540
    },
    {
      "epoch": 2.099393580831238,
      "grad_norm": 0.18551063537597656,
      "learning_rate": 0.00011612063867534004,
      "loss": 0.1728,
      "step": 3550
    },
    {
      "epoch": 2.105309865404526,
      "grad_norm": 0.14800463616847992,
      "learning_rate": 0.00011588409225310468,
      "loss": 0.1763,
      "step": 3560
    },
    {
      "epoch": 2.111226149977814,
      "grad_norm": 0.18757040798664093,
      "learning_rate": 0.00011564754583086933,
      "loss": 0.1793,
      "step": 3570
    },
    {
      "epoch": 2.117142434551102,
      "grad_norm": 0.1786022037267685,
      "learning_rate": 0.00011541099940863396,
      "loss": 0.1675,
      "step": 3580
    },
    {
      "epoch": 2.1230587191243897,
      "grad_norm": 0.19193613529205322,
      "learning_rate": 0.0001151744529863986,
      "loss": 0.1701,
      "step": 3590
    },
    {
      "epoch": 2.128975003697678,
      "grad_norm": 0.18491634726524353,
      "learning_rate": 0.00011493790656416323,
      "loss": 0.1743,
      "step": 3600
    },
    {
      "epoch": 2.134891288270966,
      "grad_norm": 0.19971312582492828,
      "learning_rate": 0.00011470136014192785,
      "loss": 0.1675,
      "step": 3610
    },
    {
      "epoch": 2.140807572844254,
      "grad_norm": 0.190127894282341,
      "learning_rate": 0.00011446481371969249,
      "loss": 0.1494,
      "step": 3620
    },
    {
      "epoch": 2.146723857417542,
      "grad_norm": 0.19566629827022552,
      "learning_rate": 0.00011422826729745712,
      "loss": 0.1605,
      "step": 3630
    },
    {
      "epoch": 2.1526401419908296,
      "grad_norm": 0.14194072782993317,
      "learning_rate": 0.00011399172087522177,
      "loss": 0.1727,
      "step": 3640
    },
    {
      "epoch": 2.1585564265641177,
      "grad_norm": 0.1764727383852005,
      "learning_rate": 0.0001137551744529864,
      "loss": 0.1804,
      "step": 3650
    },
    {
      "epoch": 2.164472711137406,
      "grad_norm": 0.18777573108673096,
      "learning_rate": 0.00011351862803075104,
      "loss": 0.1599,
      "step": 3660
    },
    {
      "epoch": 2.170388995710694,
      "grad_norm": 0.17096850275993347,
      "learning_rate": 0.00011328208160851567,
      "loss": 0.1684,
      "step": 3670
    },
    {
      "epoch": 2.1763052802839815,
      "grad_norm": 0.19295574724674225,
      "learning_rate": 0.00011304553518628032,
      "loss": 0.1578,
      "step": 3680
    },
    {
      "epoch": 2.1822215648572696,
      "grad_norm": 0.20202390849590302,
      "learning_rate": 0.00011280898876404496,
      "loss": 0.1693,
      "step": 3690
    },
    {
      "epoch": 2.1881378494305577,
      "grad_norm": 0.19899043440818787,
      "learning_rate": 0.00011257244234180959,
      "loss": 0.1638,
      "step": 3700
    },
    {
      "epoch": 2.1940541340038457,
      "grad_norm": 0.1993986815214157,
      "learning_rate": 0.00011233589591957423,
      "loss": 0.1711,
      "step": 3710
    },
    {
      "epoch": 2.1999704185771334,
      "grad_norm": 0.23884573578834534,
      "learning_rate": 0.00011209934949733886,
      "loss": 0.1659,
      "step": 3720
    },
    {
      "epoch": 2.2058867031504215,
      "grad_norm": 0.19397929310798645,
      "learning_rate": 0.00011186280307510348,
      "loss": 0.1758,
      "step": 3730
    },
    {
      "epoch": 2.2118029877237095,
      "grad_norm": 0.19556374847888947,
      "learning_rate": 0.00011162625665286813,
      "loss": 0.1693,
      "step": 3740
    },
    {
      "epoch": 2.2177192722969976,
      "grad_norm": 0.20295217633247375,
      "learning_rate": 0.00011138971023063276,
      "loss": 0.1699,
      "step": 3750
    },
    {
      "epoch": 2.2236355568702857,
      "grad_norm": 0.22993043065071106,
      "learning_rate": 0.0001111531638083974,
      "loss": 0.1812,
      "step": 3760
    },
    {
      "epoch": 2.2295518414435733,
      "grad_norm": 0.17141816020011902,
      "learning_rate": 0.00011091661738616203,
      "loss": 0.1612,
      "step": 3770
    },
    {
      "epoch": 2.2354681260168614,
      "grad_norm": 0.21445408463478088,
      "learning_rate": 0.00011068007096392667,
      "loss": 0.1861,
      "step": 3780
    },
    {
      "epoch": 2.2413844105901495,
      "grad_norm": 0.18275536596775055,
      "learning_rate": 0.00011044352454169132,
      "loss": 0.163,
      "step": 3790
    },
    {
      "epoch": 2.2473006951634376,
      "grad_norm": 0.18932732939720154,
      "learning_rate": 0.00011020697811945595,
      "loss": 0.1652,
      "step": 3800
    },
    {
      "epoch": 2.253216979736725,
      "grad_norm": 0.24179939925670624,
      "learning_rate": 0.00010997043169722059,
      "loss": 0.1736,
      "step": 3810
    },
    {
      "epoch": 2.2591332643100133,
      "grad_norm": 0.20515844225883484,
      "learning_rate": 0.00010973388527498522,
      "loss": 0.1619,
      "step": 3820
    },
    {
      "epoch": 2.2650495488833013,
      "grad_norm": 0.18975844979286194,
      "learning_rate": 0.00010949733885274987,
      "loss": 0.1595,
      "step": 3830
    },
    {
      "epoch": 2.2709658334565894,
      "grad_norm": 0.19695214927196503,
      "learning_rate": 0.0001092607924305145,
      "loss": 0.1708,
      "step": 3840
    },
    {
      "epoch": 2.276882118029877,
      "grad_norm": 0.21490958333015442,
      "learning_rate": 0.00010902424600827914,
      "loss": 0.1698,
      "step": 3850
    },
    {
      "epoch": 2.282798402603165,
      "grad_norm": 0.1880273073911667,
      "learning_rate": 0.00010878769958604376,
      "loss": 0.1684,
      "step": 3860
    },
    {
      "epoch": 2.288714687176453,
      "grad_norm": 0.2160813808441162,
      "learning_rate": 0.00010855115316380839,
      "loss": 0.1651,
      "step": 3870
    },
    {
      "epoch": 2.2946309717497413,
      "grad_norm": 0.18204712867736816,
      "learning_rate": 0.00010831460674157303,
      "loss": 0.1805,
      "step": 3880
    },
    {
      "epoch": 2.3005472563230294,
      "grad_norm": 0.18584056198596954,
      "learning_rate": 0.00010807806031933768,
      "loss": 0.1578,
      "step": 3890
    },
    {
      "epoch": 2.306463540896317,
      "grad_norm": 0.19306468963623047,
      "learning_rate": 0.00010784151389710231,
      "loss": 0.1709,
      "step": 3900
    },
    {
      "epoch": 2.312379825469605,
      "grad_norm": 0.19859866797924042,
      "learning_rate": 0.00010760496747486695,
      "loss": 0.1725,
      "step": 3910
    },
    {
      "epoch": 2.318296110042893,
      "grad_norm": 0.20035742223262787,
      "learning_rate": 0.00010736842105263158,
      "loss": 0.1734,
      "step": 3920
    },
    {
      "epoch": 2.3242123946161812,
      "grad_norm": 0.2182491421699524,
      "learning_rate": 0.00010713187463039623,
      "loss": 0.1751,
      "step": 3930
    },
    {
      "epoch": 2.330128679189469,
      "grad_norm": 0.20139285922050476,
      "learning_rate": 0.00010689532820816086,
      "loss": 0.171,
      "step": 3940
    },
    {
      "epoch": 2.336044963762757,
      "grad_norm": 0.1590910255908966,
      "learning_rate": 0.0001066587817859255,
      "loss": 0.1591,
      "step": 3950
    },
    {
      "epoch": 2.341961248336045,
      "grad_norm": 0.24259456992149353,
      "learning_rate": 0.00010642223536369013,
      "loss": 0.1776,
      "step": 3960
    },
    {
      "epoch": 2.347877532909333,
      "grad_norm": 0.18422773480415344,
      "learning_rate": 0.00010618568894145478,
      "loss": 0.1645,
      "step": 3970
    },
    {
      "epoch": 2.3537938174826207,
      "grad_norm": 0.17978371679782867,
      "learning_rate": 0.00010594914251921939,
      "loss": 0.1674,
      "step": 3980
    },
    {
      "epoch": 2.359710102055909,
      "grad_norm": 0.21216586232185364,
      "learning_rate": 0.00010571259609698402,
      "loss": 0.1752,
      "step": 3990
    },
    {
      "epoch": 2.365626386629197,
      "grad_norm": 0.22420205175876617,
      "learning_rate": 0.00010547604967474867,
      "loss": 0.1709,
      "step": 4000
    },
    {
      "epoch": 2.371542671202485,
      "grad_norm": 0.18185408413410187,
      "learning_rate": 0.00010523950325251331,
      "loss": 0.1879,
      "step": 4010
    },
    {
      "epoch": 2.377458955775773,
      "grad_norm": 0.2132907509803772,
      "learning_rate": 0.00010500295683027794,
      "loss": 0.1706,
      "step": 4020
    },
    {
      "epoch": 2.3833752403490607,
      "grad_norm": 0.212551087141037,
      "learning_rate": 0.00010476641040804259,
      "loss": 0.1902,
      "step": 4030
    },
    {
      "epoch": 2.3892915249223488,
      "grad_norm": 0.20350724458694458,
      "learning_rate": 0.00010452986398580722,
      "loss": 0.1641,
      "step": 4040
    },
    {
      "epoch": 2.395207809495637,
      "grad_norm": 0.18508796393871307,
      "learning_rate": 0.00010429331756357186,
      "loss": 0.1787,
      "step": 4050
    },
    {
      "epoch": 2.401124094068925,
      "grad_norm": 0.19346515834331512,
      "learning_rate": 0.00010405677114133649,
      "loss": 0.155,
      "step": 4060
    },
    {
      "epoch": 2.4070403786422125,
      "grad_norm": 0.18726646900177002,
      "learning_rate": 0.00010382022471910113,
      "loss": 0.1704,
      "step": 4070
    },
    {
      "epoch": 2.4129566632155006,
      "grad_norm": 0.22076137363910675,
      "learning_rate": 0.00010358367829686576,
      "loss": 0.1736,
      "step": 4080
    },
    {
      "epoch": 2.4188729477887887,
      "grad_norm": 0.20366355776786804,
      "learning_rate": 0.00010334713187463041,
      "loss": 0.1637,
      "step": 4090
    },
    {
      "epoch": 2.4247892323620768,
      "grad_norm": 0.18918476998806,
      "learning_rate": 0.00010311058545239505,
      "loss": 0.1596,
      "step": 4100
    },
    {
      "epoch": 2.4307055169353644,
      "grad_norm": 0.2161514014005661,
      "learning_rate": 0.00010287403903015967,
      "loss": 0.1744,
      "step": 4110
    },
    {
      "epoch": 2.4366218015086525,
      "grad_norm": 0.17667977511882782,
      "learning_rate": 0.0001026374926079243,
      "loss": 0.1625,
      "step": 4120
    },
    {
      "epoch": 2.4425380860819406,
      "grad_norm": 0.2180742621421814,
      "learning_rate": 0.00010240094618568894,
      "loss": 0.1585,
      "step": 4130
    },
    {
      "epoch": 2.4484543706552286,
      "grad_norm": 0.15600672364234924,
      "learning_rate": 0.00010216439976345357,
      "loss": 0.1763,
      "step": 4140
    },
    {
      "epoch": 2.4543706552285167,
      "grad_norm": 0.16925179958343506,
      "learning_rate": 0.00010192785334121822,
      "loss": 0.1691,
      "step": 4150
    },
    {
      "epoch": 2.4602869398018044,
      "grad_norm": 0.2582298517227173,
      "learning_rate": 0.00010169130691898285,
      "loss": 0.1613,
      "step": 4160
    },
    {
      "epoch": 2.4662032243750924,
      "grad_norm": 0.1958106905221939,
      "learning_rate": 0.0001014547604967475,
      "loss": 0.1764,
      "step": 4170
    },
    {
      "epoch": 2.4721195089483805,
      "grad_norm": 0.24978716671466827,
      "learning_rate": 0.00010121821407451212,
      "loss": 0.1764,
      "step": 4180
    },
    {
      "epoch": 2.4780357935216686,
      "grad_norm": 0.22608187794685364,
      "learning_rate": 0.00010098166765227677,
      "loss": 0.1661,
      "step": 4190
    },
    {
      "epoch": 2.483952078094956,
      "grad_norm": 0.1840301752090454,
      "learning_rate": 0.00010074512123004141,
      "loss": 0.1641,
      "step": 4200
    },
    {
      "epoch": 2.4898683626682443,
      "grad_norm": 0.2184632122516632,
      "learning_rate": 0.00010050857480780604,
      "loss": 0.1772,
      "step": 4210
    },
    {
      "epoch": 2.4957846472415324,
      "grad_norm": 0.20112833380699158,
      "learning_rate": 0.00010027202838557069,
      "loss": 0.1729,
      "step": 4220
    },
    {
      "epoch": 2.5017009318148204,
      "grad_norm": 0.21286800503730774,
      "learning_rate": 0.00010003548196333532,
      "loss": 0.1701,
      "step": 4230
    },
    {
      "epoch": 2.507617216388108,
      "grad_norm": 0.22979885339736938,
      "learning_rate": 9.979893554109995e-05,
      "loss": 0.1594,
      "step": 4240
    },
    {
      "epoch": 2.513533500961396,
      "grad_norm": 0.1837194859981537,
      "learning_rate": 9.956238911886459e-05,
      "loss": 0.1656,
      "step": 4250
    },
    {
      "epoch": 2.5194497855346842,
      "grad_norm": 0.24229776859283447,
      "learning_rate": 9.932584269662922e-05,
      "loss": 0.185,
      "step": 4260
    },
    {
      "epoch": 2.5253660701079723,
      "grad_norm": 0.26852405071258545,
      "learning_rate": 9.908929627439385e-05,
      "loss": 0.1592,
      "step": 4270
    },
    {
      "epoch": 2.5312823546812604,
      "grad_norm": 0.22012482583522797,
      "learning_rate": 9.885274985215848e-05,
      "loss": 0.1521,
      "step": 4280
    },
    {
      "epoch": 2.537198639254548,
      "grad_norm": 0.19420914351940155,
      "learning_rate": 9.861620342992313e-05,
      "loss": 0.1678,
      "step": 4290
    },
    {
      "epoch": 2.543114923827836,
      "grad_norm": 0.22468017041683197,
      "learning_rate": 9.837965700768777e-05,
      "loss": 0.185,
      "step": 4300
    },
    {
      "epoch": 2.549031208401124,
      "grad_norm": 0.19675326347351074,
      "learning_rate": 9.81431105854524e-05,
      "loss": 0.1748,
      "step": 4310
    },
    {
      "epoch": 2.554947492974412,
      "grad_norm": 0.23119555413722992,
      "learning_rate": 9.790656416321705e-05,
      "loss": 0.1632,
      "step": 4320
    },
    {
      "epoch": 2.5608637775477,
      "grad_norm": 0.22516991198062897,
      "learning_rate": 9.767001774098166e-05,
      "loss": 0.1641,
      "step": 4330
    },
    {
      "epoch": 2.566780062120988,
      "grad_norm": 0.2033448964357376,
      "learning_rate": 9.74334713187463e-05,
      "loss": 0.1693,
      "step": 4340
    },
    {
      "epoch": 2.572696346694276,
      "grad_norm": 0.20657308399677277,
      "learning_rate": 9.719692489651094e-05,
      "loss": 0.1794,
      "step": 4350
    },
    {
      "epoch": 2.578612631267564,
      "grad_norm": 0.24272598326206207,
      "learning_rate": 9.696037847427558e-05,
      "loss": 0.1756,
      "step": 4360
    },
    {
      "epoch": 2.5845289158408518,
      "grad_norm": 0.26004305481910706,
      "learning_rate": 9.672383205204023e-05,
      "loss": 0.1805,
      "step": 4370
    },
    {
      "epoch": 2.59044520041414,
      "grad_norm": 0.19838353991508484,
      "learning_rate": 9.648728562980486e-05,
      "loss": 0.168,
      "step": 4380
    },
    {
      "epoch": 2.596361484987428,
      "grad_norm": 0.19976334273815155,
      "learning_rate": 9.625073920756949e-05,
      "loss": 0.1711,
      "step": 4390
    },
    {
      "epoch": 2.602277769560716,
      "grad_norm": 0.17970041930675507,
      "learning_rate": 9.601419278533412e-05,
      "loss": 0.1809,
      "step": 4400
    },
    {
      "epoch": 2.608194054134004,
      "grad_norm": 0.1804213970899582,
      "learning_rate": 9.577764636309876e-05,
      "loss": 0.1733,
      "step": 4410
    },
    {
      "epoch": 2.6141103387072917,
      "grad_norm": 0.1800498366355896,
      "learning_rate": 9.55410999408634e-05,
      "loss": 0.1745,
      "step": 4420
    },
    {
      "epoch": 2.62002662328058,
      "grad_norm": 0.1661929488182068,
      "learning_rate": 9.530455351862804e-05,
      "loss": 0.1574,
      "step": 4430
    },
    {
      "epoch": 2.625942907853868,
      "grad_norm": 0.1759653240442276,
      "learning_rate": 9.506800709639268e-05,
      "loss": 0.173,
      "step": 4440
    },
    {
      "epoch": 2.6318591924271555,
      "grad_norm": 0.16300061345100403,
      "learning_rate": 9.483146067415731e-05,
      "loss": 0.1772,
      "step": 4450
    },
    {
      "epoch": 2.6377754770004436,
      "grad_norm": 0.21052640676498413,
      "learning_rate": 9.459491425192194e-05,
      "loss": 0.1766,
      "step": 4460
    },
    {
      "epoch": 2.6436917615737316,
      "grad_norm": 0.20898695290088654,
      "learning_rate": 9.435836782968658e-05,
      "loss": 0.1608,
      "step": 4470
    },
    {
      "epoch": 2.6496080461470197,
      "grad_norm": 0.23915351927280426,
      "learning_rate": 9.412182140745121e-05,
      "loss": 0.1942,
      "step": 4480
    },
    {
      "epoch": 2.655524330720308,
      "grad_norm": 0.2302398979663849,
      "learning_rate": 9.388527498521586e-05,
      "loss": 0.1662,
      "step": 4490
    },
    {
      "epoch": 2.6614406152935954,
      "grad_norm": 0.1698969155550003,
      "learning_rate": 9.364872856298049e-05,
      "loss": 0.1645,
      "step": 4500
    },
    {
      "epoch": 2.6673568998668835,
      "grad_norm": 0.18568897247314453,
      "learning_rate": 9.341218214074513e-05,
      "loss": 0.1672,
      "step": 4510
    },
    {
      "epoch": 2.6732731844401716,
      "grad_norm": 0.18279893696308136,
      "learning_rate": 9.317563571850976e-05,
      "loss": 0.1692,
      "step": 4520
    },
    {
      "epoch": 2.6791894690134597,
      "grad_norm": 0.19371260702610016,
      "learning_rate": 9.29390892962744e-05,
      "loss": 0.155,
      "step": 4530
    },
    {
      "epoch": 2.6851057535867477,
      "grad_norm": 0.23531952500343323,
      "learning_rate": 9.270254287403904e-05,
      "loss": 0.1667,
      "step": 4540
    },
    {
      "epoch": 2.6910220381600354,
      "grad_norm": 0.1629522740840912,
      "learning_rate": 9.246599645180367e-05,
      "loss": 0.1782,
      "step": 4550
    },
    {
      "epoch": 2.6969383227333235,
      "grad_norm": 0.24215397238731384,
      "learning_rate": 9.222945002956831e-05,
      "loss": 0.1676,
      "step": 4560
    },
    {
      "epoch": 2.7028546073066115,
      "grad_norm": 0.19224992394447327,
      "learning_rate": 9.199290360733294e-05,
      "loss": 0.1565,
      "step": 4570
    },
    {
      "epoch": 2.708770891879899,
      "grad_norm": 0.19978319108486176,
      "learning_rate": 9.175635718509757e-05,
      "loss": 0.1559,
      "step": 4580
    },
    {
      "epoch": 2.7146871764531872,
      "grad_norm": 0.23167409002780914,
      "learning_rate": 9.151981076286222e-05,
      "loss": 0.1669,
      "step": 4590
    },
    {
      "epoch": 2.7206034610264753,
      "grad_norm": 0.22332774102687836,
      "learning_rate": 9.128326434062685e-05,
      "loss": 0.1792,
      "step": 4600
    },
    {
      "epoch": 2.7265197455997634,
      "grad_norm": 0.18819037079811096,
      "learning_rate": 9.104671791839149e-05,
      "loss": 0.1647,
      "step": 4610
    },
    {
      "epoch": 2.7324360301730515,
      "grad_norm": 0.17301881313323975,
      "learning_rate": 9.081017149615612e-05,
      "loss": 0.1637,
      "step": 4620
    },
    {
      "epoch": 2.738352314746339,
      "grad_norm": 0.19728048145771027,
      "learning_rate": 9.057362507392077e-05,
      "loss": 0.1706,
      "step": 4630
    },
    {
      "epoch": 2.744268599319627,
      "grad_norm": 0.17882207036018372,
      "learning_rate": 9.03370786516854e-05,
      "loss": 0.1781,
      "step": 4640
    },
    {
      "epoch": 2.7501848838929153,
      "grad_norm": 0.15596726536750793,
      "learning_rate": 9.010053222945003e-05,
      "loss": 0.169,
      "step": 4650
    },
    {
      "epoch": 2.7561011684662033,
      "grad_norm": 0.1836707592010498,
      "learning_rate": 8.986398580721467e-05,
      "loss": 0.1609,
      "step": 4660
    },
    {
      "epoch": 2.7620174530394914,
      "grad_norm": 0.1901775598526001,
      "learning_rate": 8.96274393849793e-05,
      "loss": 0.1908,
      "step": 4670
    },
    {
      "epoch": 2.767933737612779,
      "grad_norm": 0.23382459580898285,
      "learning_rate": 8.939089296274395e-05,
      "loss": 0.1661,
      "step": 4680
    },
    {
      "epoch": 2.773850022186067,
      "grad_norm": 0.15012018382549286,
      "learning_rate": 8.915434654050858e-05,
      "loss": 0.1622,
      "step": 4690
    },
    {
      "epoch": 2.779766306759355,
      "grad_norm": 0.18912619352340698,
      "learning_rate": 8.891780011827322e-05,
      "loss": 0.1753,
      "step": 4700
    },
    {
      "epoch": 2.785682591332643,
      "grad_norm": 0.20293989777565002,
      "learning_rate": 8.868125369603785e-05,
      "loss": 0.182,
      "step": 4710
    },
    {
      "epoch": 2.791598875905931,
      "grad_norm": 0.21467797458171844,
      "learning_rate": 8.844470727380248e-05,
      "loss": 0.1684,
      "step": 4720
    },
    {
      "epoch": 2.797515160479219,
      "grad_norm": 0.19022944569587708,
      "learning_rate": 8.820816085156713e-05,
      "loss": 0.1666,
      "step": 4730
    },
    {
      "epoch": 2.803431445052507,
      "grad_norm": 0.1686965674161911,
      "learning_rate": 8.797161442933176e-05,
      "loss": 0.1637,
      "step": 4740
    },
    {
      "epoch": 2.809347729625795,
      "grad_norm": 0.2166498601436615,
      "learning_rate": 8.77350680070964e-05,
      "loss": 0.1665,
      "step": 4750
    },
    {
      "epoch": 2.815264014199083,
      "grad_norm": 0.19840142130851746,
      "learning_rate": 8.749852158486103e-05,
      "loss": 0.1724,
      "step": 4760
    },
    {
      "epoch": 2.821180298772371,
      "grad_norm": 0.2084844410419464,
      "learning_rate": 8.726197516262566e-05,
      "loss": 0.1584,
      "step": 4770
    },
    {
      "epoch": 2.827096583345659,
      "grad_norm": 0.19593539834022522,
      "learning_rate": 8.70254287403903e-05,
      "loss": 0.1655,
      "step": 4780
    },
    {
      "epoch": 2.833012867918947,
      "grad_norm": 0.23104257881641388,
      "learning_rate": 8.678888231815494e-05,
      "loss": 0.1792,
      "step": 4790
    },
    {
      "epoch": 2.838929152492235,
      "grad_norm": 0.18221116065979004,
      "learning_rate": 8.655233589591958e-05,
      "loss": 0.1734,
      "step": 4800
    },
    {
      "epoch": 2.8448454370655227,
      "grad_norm": 0.20636819303035736,
      "learning_rate": 8.631578947368421e-05,
      "loss": 0.1616,
      "step": 4810
    },
    {
      "epoch": 2.850761721638811,
      "grad_norm": 0.161359503865242,
      "learning_rate": 8.607924305144885e-05,
      "loss": 0.1687,
      "step": 4820
    },
    {
      "epoch": 2.856678006212099,
      "grad_norm": 0.15666793286800385,
      "learning_rate": 8.58426966292135e-05,
      "loss": 0.1635,
      "step": 4830
    },
    {
      "epoch": 2.8625942907853865,
      "grad_norm": 0.20701716840267181,
      "learning_rate": 8.560615020697812e-05,
      "loss": 0.1641,
      "step": 4840
    },
    {
      "epoch": 2.8685105753586746,
      "grad_norm": 0.2425736039876938,
      "learning_rate": 8.536960378474276e-05,
      "loss": 0.1748,
      "step": 4850
    },
    {
      "epoch": 2.8744268599319627,
      "grad_norm": 0.21001839637756348,
      "learning_rate": 8.513305736250739e-05,
      "loss": 0.18,
      "step": 4860
    },
    {
      "epoch": 2.8803431445052508,
      "grad_norm": 0.1929936408996582,
      "learning_rate": 8.489651094027203e-05,
      "loss": 0.1584,
      "step": 4870
    },
    {
      "epoch": 2.886259429078539,
      "grad_norm": 0.1935538500547409,
      "learning_rate": 8.465996451803668e-05,
      "loss": 0.1802,
      "step": 4880
    },
    {
      "epoch": 2.8921757136518265,
      "grad_norm": 0.20965643227100372,
      "learning_rate": 8.442341809580131e-05,
      "loss": 0.1666,
      "step": 4890
    },
    {
      "epoch": 2.8980919982251145,
      "grad_norm": 0.20319680869579315,
      "learning_rate": 8.418687167356594e-05,
      "loss": 0.1628,
      "step": 4900
    },
    {
      "epoch": 2.9040082827984026,
      "grad_norm": 0.25889962911605835,
      "learning_rate": 8.395032525133057e-05,
      "loss": 0.1781,
      "step": 4910
    },
    {
      "epoch": 2.9099245673716907,
      "grad_norm": 0.21430380642414093,
      "learning_rate": 8.371377882909521e-05,
      "loss": 0.1707,
      "step": 4920
    },
    {
      "epoch": 2.9158408519449788,
      "grad_norm": 0.16857098042964935,
      "learning_rate": 8.347723240685986e-05,
      "loss": 0.155,
      "step": 4930
    },
    {
      "epoch": 2.9217571365182664,
      "grad_norm": 0.23627646267414093,
      "learning_rate": 8.324068598462449e-05,
      "loss": 0.1662,
      "step": 4940
    },
    {
      "epoch": 2.9276734210915545,
      "grad_norm": 0.19414183497428894,
      "learning_rate": 8.300413956238913e-05,
      "loss": 0.1629,
      "step": 4950
    },
    {
      "epoch": 2.9335897056648426,
      "grad_norm": 0.20851437747478485,
      "learning_rate": 8.276759314015375e-05,
      "loss": 0.1735,
      "step": 4960
    },
    {
      "epoch": 2.93950599023813,
      "grad_norm": 0.1863401085138321,
      "learning_rate": 8.253104671791839e-05,
      "loss": 0.1666,
      "step": 4970
    },
    {
      "epoch": 2.9454222748114183,
      "grad_norm": 0.25693923234939575,
      "learning_rate": 8.229450029568304e-05,
      "loss": 0.1744,
      "step": 4980
    },
    {
      "epoch": 2.9513385593847064,
      "grad_norm": 0.1877080202102661,
      "learning_rate": 8.205795387344767e-05,
      "loss": 0.1745,
      "step": 4990
    },
    {
      "epoch": 2.9572548439579944,
      "grad_norm": 0.24055835604667664,
      "learning_rate": 8.182140745121231e-05,
      "loss": 0.1792,
      "step": 5000
    },
    {
      "epoch": 2.9631711285312825,
      "grad_norm": 0.21867774426937103,
      "learning_rate": 8.158486102897694e-05,
      "loss": 0.1728,
      "step": 5010
    },
    {
      "epoch": 2.96908741310457,
      "grad_norm": 0.20643459260463715,
      "learning_rate": 8.134831460674159e-05,
      "loss": 0.1596,
      "step": 5020
    },
    {
      "epoch": 2.975003697677858,
      "grad_norm": 0.21741750836372375,
      "learning_rate": 8.11117681845062e-05,
      "loss": 0.1688,
      "step": 5030
    },
    {
      "epoch": 2.9809199822511463,
      "grad_norm": 0.19863691926002502,
      "learning_rate": 8.087522176227085e-05,
      "loss": 0.153,
      "step": 5040
    },
    {
      "epoch": 2.9868362668244344,
      "grad_norm": 0.17509731650352478,
      "learning_rate": 8.063867534003549e-05,
      "loss": 0.1684,
      "step": 5050
    },
    {
      "epoch": 2.9927525513977224,
      "grad_norm": 0.1528373509645462,
      "learning_rate": 8.040212891780012e-05,
      "loss": 0.1634,
      "step": 5060
    },
    {
      "epoch": 2.99866883597101,
      "grad_norm": 0.22971637547016144,
      "learning_rate": 8.016558249556477e-05,
      "loss": 0.1778,
      "step": 5070
    },
    {
      "epoch": 3.0041413992013015,
      "grad_norm": 0.21488654613494873,
      "learning_rate": 7.99290360733294e-05,
      "loss": 0.1768,
      "step": 5080
    },
    {
      "epoch": 3.0100576837745896,
      "grad_norm": 0.18328852951526642,
      "learning_rate": 7.969248965109403e-05,
      "loss": 0.1597,
      "step": 5090
    },
    {
      "epoch": 3.0159739683478777,
      "grad_norm": 0.2998380959033966,
      "learning_rate": 7.945594322885867e-05,
      "loss": 0.1509,
      "step": 5100
    },
    {
      "epoch": 3.0218902529211653,
      "grad_norm": 0.24746021628379822,
      "learning_rate": 7.92193968066233e-05,
      "loss": 0.1676,
      "step": 5110
    },
    {
      "epoch": 3.0278065374944534,
      "grad_norm": 0.18362899124622345,
      "learning_rate": 7.898285038438794e-05,
      "loss": 0.1551,
      "step": 5120
    },
    {
      "epoch": 3.0337228220677415,
      "grad_norm": 0.21996243298053741,
      "learning_rate": 7.874630396215258e-05,
      "loss": 0.1611,
      "step": 5130
    },
    {
      "epoch": 3.0396391066410295,
      "grad_norm": 0.17321139574050903,
      "learning_rate": 7.850975753991722e-05,
      "loss": 0.1656,
      "step": 5140
    },
    {
      "epoch": 3.0455553912143176,
      "grad_norm": 0.22459116578102112,
      "learning_rate": 7.827321111768185e-05,
      "loss": 0.1611,
      "step": 5150
    },
    {
      "epoch": 3.0514716757876053,
      "grad_norm": 0.19325877726078033,
      "learning_rate": 7.803666469544648e-05,
      "loss": 0.1606,
      "step": 5160
    },
    {
      "epoch": 3.0573879603608933,
      "grad_norm": 0.34196123480796814,
      "learning_rate": 7.780011827321112e-05,
      "loss": 0.1612,
      "step": 5170
    },
    {
      "epoch": 3.0633042449341814,
      "grad_norm": 0.1903485208749771,
      "learning_rate": 7.756357185097575e-05,
      "loss": 0.1758,
      "step": 5180
    },
    {
      "epoch": 3.0692205295074695,
      "grad_norm": 0.21387621760368347,
      "learning_rate": 7.73270254287404e-05,
      "loss": 0.1541,
      "step": 5190
    },
    {
      "epoch": 3.075136814080757,
      "grad_norm": 0.18480192124843597,
      "learning_rate": 7.709047900650503e-05,
      "loss": 0.1514,
      "step": 5200
    },
    {
      "epoch": 3.081053098654045,
      "grad_norm": 0.2155197709798813,
      "learning_rate": 7.685393258426966e-05,
      "loss": 0.1628,
      "step": 5210
    },
    {
      "epoch": 3.0869693832273333,
      "grad_norm": 0.20348826050758362,
      "learning_rate": 7.66173861620343e-05,
      "loss": 0.1758,
      "step": 5220
    },
    {
      "epoch": 3.0928856678006214,
      "grad_norm": 0.205707848072052,
      "learning_rate": 7.638083973979893e-05,
      "loss": 0.1681,
      "step": 5230
    },
    {
      "epoch": 3.098801952373909,
      "grad_norm": 0.234746053814888,
      "learning_rate": 7.614429331756358e-05,
      "loss": 0.1724,
      "step": 5240
    },
    {
      "epoch": 3.104718236947197,
      "grad_norm": 0.2566410303115845,
      "learning_rate": 7.590774689532821e-05,
      "loss": 0.1476,
      "step": 5250
    },
    {
      "epoch": 3.110634521520485,
      "grad_norm": 0.2177446335554123,
      "learning_rate": 7.567120047309285e-05,
      "loss": 0.1582,
      "step": 5260
    },
    {
      "epoch": 3.116550806093773,
      "grad_norm": 0.24895015358924866,
      "learning_rate": 7.543465405085748e-05,
      "loss": 0.1693,
      "step": 5270
    },
    {
      "epoch": 3.1224670906670613,
      "grad_norm": 0.20530495047569275,
      "learning_rate": 7.519810762862211e-05,
      "loss": 0.165,
      "step": 5280
    },
    {
      "epoch": 3.128383375240349,
      "grad_norm": 0.1986432671546936,
      "learning_rate": 7.496156120638676e-05,
      "loss": 0.1451,
      "step": 5290
    },
    {
      "epoch": 3.134299659813637,
      "grad_norm": 0.20580986142158508,
      "learning_rate": 7.472501478415139e-05,
      "loss": 0.1547,
      "step": 5300
    },
    {
      "epoch": 3.140215944386925,
      "grad_norm": 0.1958889663219452,
      "learning_rate": 7.448846836191603e-05,
      "loss": 0.1578,
      "step": 5310
    },
    {
      "epoch": 3.146132228960213,
      "grad_norm": 0.25748440623283386,
      "learning_rate": 7.425192193968066e-05,
      "loss": 0.1644,
      "step": 5320
    },
    {
      "epoch": 3.152048513533501,
      "grad_norm": 0.21493951976299286,
      "learning_rate": 7.40153755174453e-05,
      "loss": 0.146,
      "step": 5330
    },
    {
      "epoch": 3.157964798106789,
      "grad_norm": 0.3195039629936218,
      "learning_rate": 7.377882909520994e-05,
      "loss": 0.1733,
      "step": 5340
    },
    {
      "epoch": 3.163881082680077,
      "grad_norm": 0.21900619566440582,
      "learning_rate": 7.354228267297457e-05,
      "loss": 0.1452,
      "step": 5350
    },
    {
      "epoch": 3.169797367253365,
      "grad_norm": 0.23224122822284698,
      "learning_rate": 7.330573625073921e-05,
      "loss": 0.162,
      "step": 5360
    },
    {
      "epoch": 3.1757136518266527,
      "grad_norm": 0.18785250186920166,
      "learning_rate": 7.306918982850384e-05,
      "loss": 0.1504,
      "step": 5370
    },
    {
      "epoch": 3.1816299363999407,
      "grad_norm": 0.2023502141237259,
      "learning_rate": 7.283264340626849e-05,
      "loss": 0.1699,
      "step": 5380
    },
    {
      "epoch": 3.187546220973229,
      "grad_norm": 0.1889811009168625,
      "learning_rate": 7.259609698403313e-05,
      "loss": 0.1484,
      "step": 5390
    },
    {
      "epoch": 3.193462505546517,
      "grad_norm": 0.25181788206100464,
      "learning_rate": 7.235955056179775e-05,
      "loss": 0.1648,
      "step": 5400
    },
    {
      "epoch": 3.1993787901198045,
      "grad_norm": 0.20535871386528015,
      "learning_rate": 7.212300413956239e-05,
      "loss": 0.1594,
      "step": 5410
    },
    {
      "epoch": 3.2052950746930926,
      "grad_norm": 0.19680750370025635,
      "learning_rate": 7.188645771732702e-05,
      "loss": 0.1624,
      "step": 5420
    },
    {
      "epoch": 3.2112113592663807,
      "grad_norm": 0.18742960691452026,
      "learning_rate": 7.164991129509167e-05,
      "loss": 0.1605,
      "step": 5430
    },
    {
      "epoch": 3.2171276438396688,
      "grad_norm": 0.19926853477954865,
      "learning_rate": 7.141336487285631e-05,
      "loss": 0.1455,
      "step": 5440
    },
    {
      "epoch": 3.223043928412957,
      "grad_norm": 0.2639855146408081,
      "learning_rate": 7.117681845062094e-05,
      "loss": 0.1623,
      "step": 5450
    },
    {
      "epoch": 3.2289602129862445,
      "grad_norm": 0.22094537317752838,
      "learning_rate": 7.094027202838558e-05,
      "loss": 0.1509,
      "step": 5460
    },
    {
      "epoch": 3.2348764975595325,
      "grad_norm": 0.20849567651748657,
      "learning_rate": 7.07037256061502e-05,
      "loss": 0.1622,
      "step": 5470
    },
    {
      "epoch": 3.2407927821328206,
      "grad_norm": 0.20339912176132202,
      "learning_rate": 7.046717918391485e-05,
      "loss": 0.1711,
      "step": 5480
    },
    {
      "epoch": 3.2467090667061087,
      "grad_norm": 0.3027710020542145,
      "learning_rate": 7.023063276167948e-05,
      "loss": 0.1507,
      "step": 5490
    },
    {
      "epoch": 3.252625351279397,
      "grad_norm": 0.21161343157291412,
      "learning_rate": 6.999408633944412e-05,
      "loss": 0.1443,
      "step": 5500
    },
    {
      "epoch": 3.2585416358526844,
      "grad_norm": 0.24374651908874512,
      "learning_rate": 6.975753991720876e-05,
      "loss": 0.1538,
      "step": 5510
    },
    {
      "epoch": 3.2644579204259725,
      "grad_norm": 0.1698945015668869,
      "learning_rate": 6.95209934949734e-05,
      "loss": 0.1614,
      "step": 5520
    },
    {
      "epoch": 3.2703742049992606,
      "grad_norm": 0.20061171054840088,
      "learning_rate": 6.928444707273802e-05,
      "loss": 0.1739,
      "step": 5530
    },
    {
      "epoch": 3.276290489572548,
      "grad_norm": 0.23396851122379303,
      "learning_rate": 6.904790065050266e-05,
      "loss": 0.1608,
      "step": 5540
    },
    {
      "epoch": 3.2822067741458363,
      "grad_norm": 0.22001591324806213,
      "learning_rate": 6.88113542282673e-05,
      "loss": 0.1575,
      "step": 5550
    },
    {
      "epoch": 3.2881230587191244,
      "grad_norm": 0.2248172163963318,
      "learning_rate": 6.857480780603194e-05,
      "loss": 0.159,
      "step": 5560
    },
    {
      "epoch": 3.2940393432924124,
      "grad_norm": 0.19803203642368317,
      "learning_rate": 6.833826138379657e-05,
      "loss": 0.1668,
      "step": 5570
    },
    {
      "epoch": 3.2999556278657005,
      "grad_norm": 0.18999546766281128,
      "learning_rate": 6.810171496156122e-05,
      "loss": 0.1691,
      "step": 5580
    },
    {
      "epoch": 3.305871912438988,
      "grad_norm": 0.15967731177806854,
      "learning_rate": 6.786516853932583e-05,
      "loss": 0.1491,
      "step": 5590
    },
    {
      "epoch": 3.3117881970122762,
      "grad_norm": 0.3070014417171478,
      "learning_rate": 6.762862211709048e-05,
      "loss": 0.1607,
      "step": 5600
    },
    {
      "epoch": 3.3177044815855643,
      "grad_norm": 0.22904331982135773,
      "learning_rate": 6.739207569485512e-05,
      "loss": 0.1714,
      "step": 5610
    },
    {
      "epoch": 3.3236207661588524,
      "grad_norm": 0.18890470266342163,
      "learning_rate": 6.715552927261975e-05,
      "loss": 0.1622,
      "step": 5620
    },
    {
      "epoch": 3.3295370507321405,
      "grad_norm": 0.20649497210979462,
      "learning_rate": 6.69189828503844e-05,
      "loss": 0.1529,
      "step": 5630
    },
    {
      "epoch": 3.335453335305428,
      "grad_norm": 0.23268650472164154,
      "learning_rate": 6.668243642814903e-05,
      "loss": 0.1629,
      "step": 5640
    },
    {
      "epoch": 3.341369619878716,
      "grad_norm": 0.24578548967838287,
      "learning_rate": 6.644589000591367e-05,
      "loss": 0.1574,
      "step": 5650
    },
    {
      "epoch": 3.3472859044520042,
      "grad_norm": 0.22549909353256226,
      "learning_rate": 6.62093435836783e-05,
      "loss": 0.1531,
      "step": 5660
    },
    {
      "epoch": 3.353202189025292,
      "grad_norm": 0.21160852909088135,
      "learning_rate": 6.597279716144293e-05,
      "loss": 0.156,
      "step": 5670
    },
    {
      "epoch": 3.35911847359858,
      "grad_norm": 0.24487292766571045,
      "learning_rate": 6.573625073920758e-05,
      "loss": 0.1562,
      "step": 5680
    },
    {
      "epoch": 3.365034758171868,
      "grad_norm": 0.23120765388011932,
      "learning_rate": 6.549970431697221e-05,
      "loss": 0.156,
      "step": 5690
    },
    {
      "epoch": 3.370951042745156,
      "grad_norm": 0.19265961647033691,
      "learning_rate": 6.526315789473685e-05,
      "loss": 0.1602,
      "step": 5700
    },
    {
      "epoch": 3.376867327318444,
      "grad_norm": 0.22435258328914642,
      "learning_rate": 6.502661147250148e-05,
      "loss": 0.1611,
      "step": 5710
    },
    {
      "epoch": 3.382783611891732,
      "grad_norm": 0.27548372745513916,
      "learning_rate": 6.479006505026611e-05,
      "loss": 0.1729,
      "step": 5720
    },
    {
      "epoch": 3.38869989646502,
      "grad_norm": 0.18432867527008057,
      "learning_rate": 6.455351862803076e-05,
      "loss": 0.1567,
      "step": 5730
    },
    {
      "epoch": 3.394616181038308,
      "grad_norm": 0.24394606053829193,
      "learning_rate": 6.431697220579539e-05,
      "loss": 0.1678,
      "step": 5740
    },
    {
      "epoch": 3.400532465611596,
      "grad_norm": 0.22382070124149323,
      "learning_rate": 6.408042578356003e-05,
      "loss": 0.1508,
      "step": 5750
    },
    {
      "epoch": 3.406448750184884,
      "grad_norm": 0.23384763300418854,
      "learning_rate": 6.384387936132466e-05,
      "loss": 0.1561,
      "step": 5760
    },
    {
      "epoch": 3.4123650347581718,
      "grad_norm": 0.17863193154335022,
      "learning_rate": 6.36073329390893e-05,
      "loss": 0.1589,
      "step": 5770
    },
    {
      "epoch": 3.41828131933146,
      "grad_norm": 0.2368132323026657,
      "learning_rate": 6.337078651685394e-05,
      "loss": 0.1553,
      "step": 5780
    },
    {
      "epoch": 3.424197603904748,
      "grad_norm": 0.23515914380550385,
      "learning_rate": 6.313424009461857e-05,
      "loss": 0.1632,
      "step": 5790
    },
    {
      "epoch": 3.4301138884780356,
      "grad_norm": 0.23842772841453552,
      "learning_rate": 6.289769367238321e-05,
      "loss": 0.1772,
      "step": 5800
    },
    {
      "epoch": 3.4360301730513236,
      "grad_norm": 0.22848981618881226,
      "learning_rate": 6.266114725014784e-05,
      "loss": 0.1758,
      "step": 5810
    },
    {
      "epoch": 3.4419464576246117,
      "grad_norm": 0.2953758239746094,
      "learning_rate": 6.242460082791248e-05,
      "loss": 0.1664,
      "step": 5820
    },
    {
      "epoch": 3.4478627421979,
      "grad_norm": 0.22438248991966248,
      "learning_rate": 6.218805440567712e-05,
      "loss": 0.1606,
      "step": 5830
    },
    {
      "epoch": 3.453779026771188,
      "grad_norm": 0.2257714867591858,
      "learning_rate": 6.195150798344176e-05,
      "loss": 0.1538,
      "step": 5840
    },
    {
      "epoch": 3.4596953113444755,
      "grad_norm": 0.2606804072856903,
      "learning_rate": 6.171496156120639e-05,
      "loss": 0.1587,
      "step": 5850
    },
    {
      "epoch": 3.4656115959177636,
      "grad_norm": 0.20931729674339294,
      "learning_rate": 6.147841513897102e-05,
      "loss": 0.1664,
      "step": 5860
    },
    {
      "epoch": 3.4715278804910517,
      "grad_norm": 0.25789710879325867,
      "learning_rate": 6.124186871673566e-05,
      "loss": 0.1531,
      "step": 5870
    },
    {
      "epoch": 3.4774441650643397,
      "grad_norm": 0.24575550854206085,
      "learning_rate": 6.10053222945003e-05,
      "loss": 0.165,
      "step": 5880
    },
    {
      "epoch": 3.483360449637628,
      "grad_norm": 0.24319906532764435,
      "learning_rate": 6.076877587226494e-05,
      "loss": 0.1516,
      "step": 5890
    },
    {
      "epoch": 3.4892767342109154,
      "grad_norm": 0.22071368992328644,
      "learning_rate": 6.0532229450029576e-05,
      "loss": 0.1607,
      "step": 5900
    },
    {
      "epoch": 3.4951930187842035,
      "grad_norm": 0.18667075037956238,
      "learning_rate": 6.02956830277942e-05,
      "loss": 0.1533,
      "step": 5910
    },
    {
      "epoch": 3.5011093033574916,
      "grad_norm": 0.24472208321094513,
      "learning_rate": 6.0059136605558844e-05,
      "loss": 0.1492,
      "step": 5920
    },
    {
      "epoch": 3.5070255879307792,
      "grad_norm": 0.22336417436599731,
      "learning_rate": 5.982259018332348e-05,
      "loss": 0.1557,
      "step": 5930
    },
    {
      "epoch": 3.5129418725040673,
      "grad_norm": 0.25480717420578003,
      "learning_rate": 5.958604376108812e-05,
      "loss": 0.1528,
      "step": 5940
    },
    {
      "epoch": 3.5188581570773554,
      "grad_norm": 0.2744863033294678,
      "learning_rate": 5.9349497338852756e-05,
      "loss": 0.1635,
      "step": 5950
    },
    {
      "epoch": 3.5247744416506435,
      "grad_norm": 0.22828549146652222,
      "learning_rate": 5.911295091661739e-05,
      "loss": 0.1637,
      "step": 5960
    },
    {
      "epoch": 3.5306907262239315,
      "grad_norm": 0.29878538846969604,
      "learning_rate": 5.8876404494382023e-05,
      "loss": 0.1672,
      "step": 5970
    },
    {
      "epoch": 3.536607010797219,
      "grad_norm": 0.19337107241153717,
      "learning_rate": 5.863985807214666e-05,
      "loss": 0.1662,
      "step": 5980
    },
    {
      "epoch": 3.5425232953705073,
      "grad_norm": 0.18205790221691132,
      "learning_rate": 5.84033116499113e-05,
      "loss": 0.1644,
      "step": 5990
    },
    {
      "epoch": 3.5484395799437953,
      "grad_norm": 0.28233766555786133,
      "learning_rate": 5.8166765227675935e-05,
      "loss": 0.16,
      "step": 6000
    },
    {
      "epoch": 3.5543558645170834,
      "grad_norm": 0.22004376351833344,
      "learning_rate": 5.793021880544057e-05,
      "loss": 0.1622,
      "step": 6010
    },
    {
      "epoch": 3.5602721490903715,
      "grad_norm": 0.24186593294143677,
      "learning_rate": 5.769367238320521e-05,
      "loss": 0.1768,
      "step": 6020
    },
    {
      "epoch": 3.566188433663659,
      "grad_norm": 0.2385224550962448,
      "learning_rate": 5.745712596096984e-05,
      "loss": 0.1663,
      "step": 6030
    },
    {
      "epoch": 3.572104718236947,
      "grad_norm": 0.23181982338428497,
      "learning_rate": 5.722057953873448e-05,
      "loss": 0.1634,
      "step": 6040
    },
    {
      "epoch": 3.5780210028102353,
      "grad_norm": 0.23181672394275665,
      "learning_rate": 5.6984033116499115e-05,
      "loss": 0.1514,
      "step": 6050
    },
    {
      "epoch": 3.583937287383523,
      "grad_norm": 0.20150206983089447,
      "learning_rate": 5.674748669426375e-05,
      "loss": 0.1503,
      "step": 6060
    },
    {
      "epoch": 3.589853571956811,
      "grad_norm": 0.2299436330795288,
      "learning_rate": 5.651094027202839e-05,
      "loss": 0.1627,
      "step": 6070
    },
    {
      "epoch": 3.595769856530099,
      "grad_norm": 0.2229686975479126,
      "learning_rate": 5.6274393849793026e-05,
      "loss": 0.1725,
      "step": 6080
    },
    {
      "epoch": 3.601686141103387,
      "grad_norm": 0.22152179479599,
      "learning_rate": 5.6037847427557664e-05,
      "loss": 0.1572,
      "step": 6090
    },
    {
      "epoch": 3.607602425676675,
      "grad_norm": 0.2699390947818756,
      "learning_rate": 5.5801301005322294e-05,
      "loss": 0.16,
      "step": 6100
    },
    {
      "epoch": 3.613518710249963,
      "grad_norm": 0.22294196486473083,
      "learning_rate": 5.556475458308693e-05,
      "loss": 0.1685,
      "step": 6110
    },
    {
      "epoch": 3.619434994823251,
      "grad_norm": 0.254362553358078,
      "learning_rate": 5.532820816085157e-05,
      "loss": 0.1629,
      "step": 6120
    },
    {
      "epoch": 3.625351279396539,
      "grad_norm": 0.21236440539360046,
      "learning_rate": 5.5091661738616206e-05,
      "loss": 0.1602,
      "step": 6130
    },
    {
      "epoch": 3.631267563969827,
      "grad_norm": 0.21866486966609955,
      "learning_rate": 5.485511531638084e-05,
      "loss": 0.1522,
      "step": 6140
    },
    {
      "epoch": 3.637183848543115,
      "grad_norm": 0.3093152344226837,
      "learning_rate": 5.461856889414548e-05,
      "loss": 0.1622,
      "step": 6150
    },
    {
      "epoch": 3.643100133116403,
      "grad_norm": 0.26576536893844604,
      "learning_rate": 5.438202247191011e-05,
      "loss": 0.1604,
      "step": 6160
    },
    {
      "epoch": 3.649016417689691,
      "grad_norm": 0.28918370604515076,
      "learning_rate": 5.414547604967475e-05,
      "loss": 0.1768,
      "step": 6170
    },
    {
      "epoch": 3.654932702262979,
      "grad_norm": 0.24561361968517303,
      "learning_rate": 5.3908929627439385e-05,
      "loss": 0.1709,
      "step": 6180
    },
    {
      "epoch": 3.6608489868362666,
      "grad_norm": 0.18806661665439606,
      "learning_rate": 5.367238320520402e-05,
      "loss": 0.1538,
      "step": 6190
    },
    {
      "epoch": 3.6667652714095547,
      "grad_norm": 0.22737336158752441,
      "learning_rate": 5.343583678296866e-05,
      "loss": 0.1602,
      "step": 6200
    },
    {
      "epoch": 3.6726815559828427,
      "grad_norm": 0.2553480267524719,
      "learning_rate": 5.31992903607333e-05,
      "loss": 0.1561,
      "step": 6210
    },
    {
      "epoch": 3.678597840556131,
      "grad_norm": 0.29157838225364685,
      "learning_rate": 5.296274393849793e-05,
      "loss": 0.1648,
      "step": 6220
    },
    {
      "epoch": 3.684514125129419,
      "grad_norm": 0.2840428948402405,
      "learning_rate": 5.2726197516262565e-05,
      "loss": 0.1681,
      "step": 6230
    },
    {
      "epoch": 3.6904304097027065,
      "grad_norm": 0.27054476737976074,
      "learning_rate": 5.24896510940272e-05,
      "loss": 0.1625,
      "step": 6240
    },
    {
      "epoch": 3.6963466942759946,
      "grad_norm": 0.30943387746810913,
      "learning_rate": 5.225310467179184e-05,
      "loss": 0.1676,
      "step": 6250
    },
    {
      "epoch": 3.7022629788492827,
      "grad_norm": 0.19714447855949402,
      "learning_rate": 5.201655824955648e-05,
      "loss": 0.1653,
      "step": 6260
    },
    {
      "epoch": 3.7081792634225708,
      "grad_norm": 0.27464061975479126,
      "learning_rate": 5.178001182732112e-05,
      "loss": 0.1754,
      "step": 6270
    },
    {
      "epoch": 3.714095547995859,
      "grad_norm": 0.25327974557876587,
      "learning_rate": 5.154346540508576e-05,
      "loss": 0.1584,
      "step": 6280
    },
    {
      "epoch": 3.7200118325691465,
      "grad_norm": 0.24191172420978546,
      "learning_rate": 5.130691898285038e-05,
      "loss": 0.1626,
      "step": 6290
    },
    {
      "epoch": 3.7259281171424345,
      "grad_norm": 0.20991361141204834,
      "learning_rate": 5.107037256061502e-05,
      "loss": 0.156,
      "step": 6300
    },
    {
      "epoch": 3.7318444017157226,
      "grad_norm": 0.20598849654197693,
      "learning_rate": 5.0833826138379656e-05,
      "loss": 0.1658,
      "step": 6310
    },
    {
      "epoch": 3.7377606862890103,
      "grad_norm": 0.2755764126777649,
      "learning_rate": 5.0597279716144293e-05,
      "loss": 0.1649,
      "step": 6320
    },
    {
      "epoch": 3.7436769708622983,
      "grad_norm": 0.2462874948978424,
      "learning_rate": 5.036073329390894e-05,
      "loss": 0.1515,
      "step": 6330
    },
    {
      "epoch": 3.7495932554355864,
      "grad_norm": 0.21871373057365417,
      "learning_rate": 5.0124186871673575e-05,
      "loss": 0.1475,
      "step": 6340
    },
    {
      "epoch": 3.7555095400088745,
      "grad_norm": 0.2764486074447632,
      "learning_rate": 4.9887640449438205e-05,
      "loss": 0.1696,
      "step": 6350
    },
    {
      "epoch": 3.7614258245821626,
      "grad_norm": 0.18742191791534424,
      "learning_rate": 4.965109402720284e-05,
      "loss": 0.1495,
      "step": 6360
    },
    {
      "epoch": 3.76734210915545,
      "grad_norm": 0.26686590909957886,
      "learning_rate": 4.941454760496747e-05,
      "loss": 0.1657,
      "step": 6370
    },
    {
      "epoch": 3.7732583937287383,
      "grad_norm": 0.20178881287574768,
      "learning_rate": 4.917800118273212e-05,
      "loss": 0.163,
      "step": 6380
    },
    {
      "epoch": 3.7791746783020264,
      "grad_norm": 0.212154820561409,
      "learning_rate": 4.8941454760496754e-05,
      "loss": 0.1542,
      "step": 6390
    },
    {
      "epoch": 3.7850909628753144,
      "grad_norm": 0.21710871160030365,
      "learning_rate": 4.8704908338261385e-05,
      "loss": 0.1495,
      "step": 6400
    },
    {
      "epoch": 3.7910072474486025,
      "grad_norm": 0.23872661590576172,
      "learning_rate": 4.846836191602602e-05,
      "loss": 0.1554,
      "step": 6410
    },
    {
      "epoch": 3.79692353202189,
      "grad_norm": 0.21739664673805237,
      "learning_rate": 4.823181549379066e-05,
      "loss": 0.1448,
      "step": 6420
    },
    {
      "epoch": 3.8028398165951782,
      "grad_norm": 0.22768184542655945,
      "learning_rate": 4.7995269071555297e-05,
      "loss": 0.1616,
      "step": 6430
    },
    {
      "epoch": 3.8087561011684663,
      "grad_norm": 0.24731619656085968,
      "learning_rate": 4.7758722649319934e-05,
      "loss": 0.165,
      "step": 6440
    },
    {
      "epoch": 3.814672385741754,
      "grad_norm": 0.17834268510341644,
      "learning_rate": 4.752217622708457e-05,
      "loss": 0.1598,
      "step": 6450
    },
    {
      "epoch": 3.820588670315042,
      "grad_norm": 0.21174249053001404,
      "learning_rate": 4.72856298048492e-05,
      "loss": 0.1549,
      "step": 6460
    },
    {
      "epoch": 3.82650495488833,
      "grad_norm": 0.24732638895511627,
      "learning_rate": 4.704908338261384e-05,
      "loss": 0.1576,
      "step": 6470
    },
    {
      "epoch": 3.832421239461618,
      "grad_norm": 0.1906193643808365,
      "learning_rate": 4.6812536960378476e-05,
      "loss": 0.1608,
      "step": 6480
    },
    {
      "epoch": 3.8383375240349062,
      "grad_norm": 0.22230574488639832,
      "learning_rate": 4.657599053814311e-05,
      "loss": 0.1593,
      "step": 6490
    },
    {
      "epoch": 3.844253808608194,
      "grad_norm": 0.2300032526254654,
      "learning_rate": 4.633944411590775e-05,
      "loss": 0.1629,
      "step": 6500
    },
    {
      "epoch": 3.850170093181482,
      "grad_norm": 0.20996059477329254,
      "learning_rate": 4.610289769367239e-05,
      "loss": 0.1564,
      "step": 6510
    },
    {
      "epoch": 3.85608637775477,
      "grad_norm": 0.24450697004795074,
      "learning_rate": 4.586635127143702e-05,
      "loss": 0.1539,
      "step": 6520
    },
    {
      "epoch": 3.862002662328058,
      "grad_norm": 0.26158469915390015,
      "learning_rate": 4.5629804849201656e-05,
      "loss": 0.1635,
      "step": 6530
    },
    {
      "epoch": 3.867918946901346,
      "grad_norm": 0.1788080781698227,
      "learning_rate": 4.539325842696629e-05,
      "loss": 0.1456,
      "step": 6540
    },
    {
      "epoch": 3.873835231474634,
      "grad_norm": 0.20935560762882233,
      "learning_rate": 4.515671200473093e-05,
      "loss": 0.1571,
      "step": 6550
    },
    {
      "epoch": 3.879751516047922,
      "grad_norm": 0.2590598165988922,
      "learning_rate": 4.492016558249557e-05,
      "loss": 0.1542,
      "step": 6560
    },
    {
      "epoch": 3.88566780062121,
      "grad_norm": 0.19338999688625336,
      "learning_rate": 4.4683619160260205e-05,
      "loss": 0.1539,
      "step": 6570
    },
    {
      "epoch": 3.8915840851944976,
      "grad_norm": 0.2265738844871521,
      "learning_rate": 4.444707273802484e-05,
      "loss": 0.1523,
      "step": 6580
    },
    {
      "epoch": 3.8975003697677857,
      "grad_norm": 0.24758368730545044,
      "learning_rate": 4.421052631578947e-05,
      "loss": 0.1499,
      "step": 6590
    },
    {
      "epoch": 3.9034166543410738,
      "grad_norm": 0.21615877747535706,
      "learning_rate": 4.397397989355411e-05,
      "loss": 0.1601,
      "step": 6600
    },
    {
      "epoch": 3.909332938914362,
      "grad_norm": 0.3196428716182709,
      "learning_rate": 4.3737433471318754e-05,
      "loss": 0.173,
      "step": 6610
    },
    {
      "epoch": 3.91524922348765,
      "grad_norm": 0.220210000872612,
      "learning_rate": 4.3500887049083384e-05,
      "loss": 0.1611,
      "step": 6620
    },
    {
      "epoch": 3.9211655080609376,
      "grad_norm": 0.21159635484218597,
      "learning_rate": 4.326434062684802e-05,
      "loss": 0.1588,
      "step": 6630
    },
    {
      "epoch": 3.9270817926342256,
      "grad_norm": 0.23507584631443024,
      "learning_rate": 4.302779420461266e-05,
      "loss": 0.1625,
      "step": 6640
    },
    {
      "epoch": 3.9329980772075137,
      "grad_norm": 0.22716066241264343,
      "learning_rate": 4.279124778237729e-05,
      "loss": 0.158,
      "step": 6650
    },
    {
      "epoch": 3.938914361780802,
      "grad_norm": 0.23333559930324554,
      "learning_rate": 4.255470136014193e-05,
      "loss": 0.1455,
      "step": 6660
    },
    {
      "epoch": 3.94483064635409,
      "grad_norm": 0.2185647338628769,
      "learning_rate": 4.231815493790657e-05,
      "loss": 0.161,
      "step": 6670
    },
    {
      "epoch": 3.9507469309273775,
      "grad_norm": 0.204822838306427,
      "learning_rate": 4.20816085156712e-05,
      "loss": 0.1583,
      "step": 6680
    },
    {
      "epoch": 3.9566632155006656,
      "grad_norm": 0.18594296276569366,
      "learning_rate": 4.184506209343584e-05,
      "loss": 0.1677,
      "step": 6690
    },
    {
      "epoch": 3.9625795000739537,
      "grad_norm": 0.2408870905637741,
      "learning_rate": 4.1608515671200475e-05,
      "loss": 0.1669,
      "step": 6700
    },
    {
      "epoch": 3.9684957846472413,
      "grad_norm": 0.2321578711271286,
      "learning_rate": 4.1371969248965106e-05,
      "loss": 0.1661,
      "step": 6710
    },
    {
      "epoch": 3.9744120692205294,
      "grad_norm": 0.21275664865970612,
      "learning_rate": 4.113542282672975e-05,
      "loss": 0.1574,
      "step": 6720
    },
    {
      "epoch": 3.9803283537938174,
      "grad_norm": 0.2354658842086792,
      "learning_rate": 4.089887640449439e-05,
      "loss": 0.1837,
      "step": 6730
    },
    {
      "epoch": 3.9862446383671055,
      "grad_norm": 0.2276725471019745,
      "learning_rate": 4.066232998225902e-05,
      "loss": 0.1563,
      "step": 6740
    },
    {
      "epoch": 3.9921609229403936,
      "grad_norm": 0.1918933093547821,
      "learning_rate": 4.0425783560023655e-05,
      "loss": 0.1527,
      "step": 6750
    },
    {
      "epoch": 3.9980772075136812,
      "grad_norm": 0.21594618260860443,
      "learning_rate": 4.018923713778829e-05,
      "loss": 0.156,
      "step": 6760
    },
    {
      "epoch": 4.003549770743973,
      "grad_norm": 0.21663875877857208,
      "learning_rate": 3.995269071555293e-05,
      "loss": 0.17,
      "step": 6770
    },
    {
      "epoch": 4.009466055317261,
      "grad_norm": 0.2146429717540741,
      "learning_rate": 3.971614429331757e-05,
      "loss": 0.1491,
      "step": 6780
    },
    {
      "epoch": 4.015382339890548,
      "grad_norm": 0.20916248857975006,
      "learning_rate": 3.9479597871082204e-05,
      "loss": 0.1542,
      "step": 6790
    },
    {
      "epoch": 4.021298624463837,
      "grad_norm": 0.20761875808238983,
      "learning_rate": 3.924305144884684e-05,
      "loss": 0.1461,
      "step": 6800
    },
    {
      "epoch": 4.0272149090371245,
      "grad_norm": 0.22944505512714386,
      "learning_rate": 3.900650502661147e-05,
      "loss": 0.1593,
      "step": 6810
    },
    {
      "epoch": 4.033131193610413,
      "grad_norm": 0.1999843567609787,
      "learning_rate": 3.876995860437611e-05,
      "loss": 0.1564,
      "step": 6820
    },
    {
      "epoch": 4.039047478183701,
      "grad_norm": 0.2298850268125534,
      "learning_rate": 3.8533412182140746e-05,
      "loss": 0.1347,
      "step": 6830
    },
    {
      "epoch": 4.044963762756988,
      "grad_norm": 0.23696158826351166,
      "learning_rate": 3.829686575990538e-05,
      "loss": 0.1573,
      "step": 6840
    },
    {
      "epoch": 4.050880047330277,
      "grad_norm": 0.31710174679756165,
      "learning_rate": 3.806031933767002e-05,
      "loss": 0.1601,
      "step": 6850
    },
    {
      "epoch": 4.0567963319035645,
      "grad_norm": 0.3221518099308014,
      "learning_rate": 3.782377291543466e-05,
      "loss": 0.1519,
      "step": 6860
    },
    {
      "epoch": 4.062712616476852,
      "grad_norm": 0.1998976171016693,
      "learning_rate": 3.758722649319929e-05,
      "loss": 0.1467,
      "step": 6870
    },
    {
      "epoch": 4.068628901050141,
      "grad_norm": 0.20850032567977905,
      "learning_rate": 3.7350680070963926e-05,
      "loss": 0.1431,
      "step": 6880
    },
    {
      "epoch": 4.074545185623428,
      "grad_norm": 0.24861888587474823,
      "learning_rate": 3.711413364872856e-05,
      "loss": 0.152,
      "step": 6890
    },
    {
      "epoch": 4.080461470196717,
      "grad_norm": 0.2234257012605667,
      "learning_rate": 3.68775872264932e-05,
      "loss": 0.1432,
      "step": 6900
    },
    {
      "epoch": 4.086377754770004,
      "grad_norm": 0.23455941677093506,
      "learning_rate": 3.664104080425784e-05,
      "loss": 0.1436,
      "step": 6910
    },
    {
      "epoch": 4.092294039343292,
      "grad_norm": 0.21420536935329437,
      "learning_rate": 3.6404494382022475e-05,
      "loss": 0.1533,
      "step": 6920
    },
    {
      "epoch": 4.098210323916581,
      "grad_norm": 0.2731505334377289,
      "learning_rate": 3.6167947959787105e-05,
      "loss": 0.1597,
      "step": 6930
    },
    {
      "epoch": 4.104126608489868,
      "grad_norm": 0.235812246799469,
      "learning_rate": 3.593140153755174e-05,
      "loss": 0.1591,
      "step": 6940
    },
    {
      "epoch": 4.110042893063157,
      "grad_norm": 0.25995925068855286,
      "learning_rate": 3.5694855115316386e-05,
      "loss": 0.1593,
      "step": 6950
    },
    {
      "epoch": 4.115959177636444,
      "grad_norm": 0.2858951985836029,
      "learning_rate": 3.5458308693081024e-05,
      "loss": 0.1628,
      "step": 6960
    },
    {
      "epoch": 4.121875462209732,
      "grad_norm": 0.22013245522975922,
      "learning_rate": 3.5221762270845654e-05,
      "loss": 0.1436,
      "step": 6970
    },
    {
      "epoch": 4.1277917467830205,
      "grad_norm": 0.22213433682918549,
      "learning_rate": 3.498521584861029e-05,
      "loss": 0.1498,
      "step": 6980
    },
    {
      "epoch": 4.133708031356308,
      "grad_norm": 0.2228090465068817,
      "learning_rate": 3.474866942637493e-05,
      "loss": 0.1568,
      "step": 6990
    },
    {
      "epoch": 4.139624315929596,
      "grad_norm": 0.22499646246433258,
      "learning_rate": 3.4512123004139566e-05,
      "loss": 0.1523,
      "step": 7000
    },
    {
      "epoch": 4.145540600502884,
      "grad_norm": 0.280133455991745,
      "learning_rate": 3.42755765819042e-05,
      "loss": 0.1597,
      "step": 7010
    },
    {
      "epoch": 4.151456885076172,
      "grad_norm": 0.22201496362686157,
      "learning_rate": 3.403903015966884e-05,
      "loss": 0.1436,
      "step": 7020
    },
    {
      "epoch": 4.1573731696494605,
      "grad_norm": 0.23622846603393555,
      "learning_rate": 3.380248373743347e-05,
      "loss": 0.1531,
      "step": 7030
    },
    {
      "epoch": 4.163289454222748,
      "grad_norm": 0.26459136605262756,
      "learning_rate": 3.356593731519811e-05,
      "loss": 0.1487,
      "step": 7040
    },
    {
      "epoch": 4.169205738796036,
      "grad_norm": 0.2411215603351593,
      "learning_rate": 3.3329390892962745e-05,
      "loss": 0.1446,
      "step": 7050
    },
    {
      "epoch": 4.175122023369324,
      "grad_norm": 0.2546432614326477,
      "learning_rate": 3.309284447072738e-05,
      "loss": 0.1644,
      "step": 7060
    },
    {
      "epoch": 4.181038307942612,
      "grad_norm": 0.18603487312793732,
      "learning_rate": 3.285629804849202e-05,
      "loss": 0.1457,
      "step": 7070
    },
    {
      "epoch": 4.1869545925159,
      "grad_norm": 0.22333264350891113,
      "learning_rate": 3.261975162625666e-05,
      "loss": 0.1478,
      "step": 7080
    },
    {
      "epoch": 4.192870877089188,
      "grad_norm": 0.24342739582061768,
      "learning_rate": 3.238320520402129e-05,
      "loss": 0.1672,
      "step": 7090
    },
    {
      "epoch": 4.198787161662476,
      "grad_norm": 0.20436248183250427,
      "learning_rate": 3.2146658781785925e-05,
      "loss": 0.1477,
      "step": 7100
    },
    {
      "epoch": 4.204703446235764,
      "grad_norm": 0.21998117864131927,
      "learning_rate": 3.191011235955056e-05,
      "loss": 0.1569,
      "step": 7110
    },
    {
      "epoch": 4.210619730809052,
      "grad_norm": 0.22339607775211334,
      "learning_rate": 3.16735659373152e-05,
      "loss": 0.1548,
      "step": 7120
    },
    {
      "epoch": 4.2165360153823395,
      "grad_norm": 0.23159372806549072,
      "learning_rate": 3.143701951507984e-05,
      "loss": 0.1469,
      "step": 7130
    },
    {
      "epoch": 4.222452299955628,
      "grad_norm": 0.3711462616920471,
      "learning_rate": 3.1200473092844474e-05,
      "loss": 0.1546,
      "step": 7140
    },
    {
      "epoch": 4.228368584528916,
      "grad_norm": 0.2546837329864502,
      "learning_rate": 3.0963926670609104e-05,
      "loss": 0.1552,
      "step": 7150
    },
    {
      "epoch": 4.234284869102204,
      "grad_norm": 0.297659307718277,
      "learning_rate": 3.072738024837374e-05,
      "loss": 0.1573,
      "step": 7160
    },
    {
      "epoch": 4.240201153675492,
      "grad_norm": 0.207992821931839,
      "learning_rate": 3.0490833826138382e-05,
      "loss": 0.1549,
      "step": 7170
    },
    {
      "epoch": 4.246117438248779,
      "grad_norm": 0.23517923057079315,
      "learning_rate": 3.025428740390302e-05,
      "loss": 0.1502,
      "step": 7180
    },
    {
      "epoch": 4.252033722822068,
      "grad_norm": 0.2401667684316635,
      "learning_rate": 3.0017740981667653e-05,
      "loss": 0.1567,
      "step": 7190
    },
    {
      "epoch": 4.257950007395356,
      "grad_norm": 0.2469712346792221,
      "learning_rate": 2.978119455943229e-05,
      "loss": 0.15,
      "step": 7200
    },
    {
      "epoch": 4.263866291968644,
      "grad_norm": 0.24459731578826904,
      "learning_rate": 2.9544648137196928e-05,
      "loss": 0.1569,
      "step": 7210
    },
    {
      "epoch": 4.269782576541932,
      "grad_norm": 0.30983150005340576,
      "learning_rate": 2.9308101714961562e-05,
      "loss": 0.1626,
      "step": 7220
    },
    {
      "epoch": 4.275698861115219,
      "grad_norm": 0.27861660718917847,
      "learning_rate": 2.90715552927262e-05,
      "loss": 0.1549,
      "step": 7230
    },
    {
      "epoch": 4.281615145688508,
      "grad_norm": 0.22781896591186523,
      "learning_rate": 2.8835008870490836e-05,
      "loss": 0.1398,
      "step": 7240
    },
    {
      "epoch": 4.2875314302617955,
      "grad_norm": 0.2553938031196594,
      "learning_rate": 2.859846244825547e-05,
      "loss": 0.1392,
      "step": 7250
    },
    {
      "epoch": 4.293447714835084,
      "grad_norm": 0.251929372549057,
      "learning_rate": 2.8361916026020107e-05,
      "loss": 0.1543,
      "step": 7260
    },
    {
      "epoch": 4.299363999408372,
      "grad_norm": 0.30592119693756104,
      "learning_rate": 2.8125369603784745e-05,
      "loss": 0.1539,
      "step": 7270
    },
    {
      "epoch": 4.305280283981659,
      "grad_norm": 0.2813035845756531,
      "learning_rate": 2.788882318154938e-05,
      "loss": 0.1583,
      "step": 7280
    },
    {
      "epoch": 4.311196568554948,
      "grad_norm": 0.22960810363292694,
      "learning_rate": 2.7652276759314016e-05,
      "loss": 0.1672,
      "step": 7290
    },
    {
      "epoch": 4.3171128531282355,
      "grad_norm": 0.24396072328090668,
      "learning_rate": 2.7415730337078653e-05,
      "loss": 0.1493,
      "step": 7300
    },
    {
      "epoch": 4.323029137701523,
      "grad_norm": 0.20763836801052094,
      "learning_rate": 2.7179183914843287e-05,
      "loss": 0.1529,
      "step": 7310
    },
    {
      "epoch": 4.328945422274812,
      "grad_norm": 0.2182939350605011,
      "learning_rate": 2.6942637492607924e-05,
      "loss": 0.158,
      "step": 7320
    },
    {
      "epoch": 4.334861706848099,
      "grad_norm": 0.19764316082000732,
      "learning_rate": 2.670609107037256e-05,
      "loss": 0.1515,
      "step": 7330
    },
    {
      "epoch": 4.340777991421388,
      "grad_norm": 0.2628207206726074,
      "learning_rate": 2.6469544648137195e-05,
      "loss": 0.1438,
      "step": 7340
    },
    {
      "epoch": 4.346694275994675,
      "grad_norm": 0.21273165941238403,
      "learning_rate": 2.6232998225901833e-05,
      "loss": 0.1547,
      "step": 7350
    },
    {
      "epoch": 4.352610560567963,
      "grad_norm": 0.2585117518901825,
      "learning_rate": 2.5996451803666473e-05,
      "loss": 0.1574,
      "step": 7360
    },
    {
      "epoch": 4.3585268451412515,
      "grad_norm": 0.2470235973596573,
      "learning_rate": 2.575990538143111e-05,
      "loss": 0.1582,
      "step": 7370
    },
    {
      "epoch": 4.364443129714539,
      "grad_norm": 0.2083352506160736,
      "learning_rate": 2.552335895919574e-05,
      "loss": 0.1471,
      "step": 7380
    },
    {
      "epoch": 4.370359414287828,
      "grad_norm": 0.2816314697265625,
      "learning_rate": 2.528681253696038e-05,
      "loss": 0.1556,
      "step": 7390
    },
    {
      "epoch": 4.376275698861115,
      "grad_norm": 0.2651142477989197,
      "learning_rate": 2.505026611472502e-05,
      "loss": 0.1536,
      "step": 7400
    },
    {
      "epoch": 4.382191983434403,
      "grad_norm": 0.2513507604598999,
      "learning_rate": 2.4813719692489653e-05,
      "loss": 0.1582,
      "step": 7410
    },
    {
      "epoch": 4.3881082680076915,
      "grad_norm": 0.19862905144691467,
      "learning_rate": 2.457717327025429e-05,
      "loss": 0.1641,
      "step": 7420
    },
    {
      "epoch": 4.394024552580979,
      "grad_norm": 0.32247471809387207,
      "learning_rate": 2.4340626848018924e-05,
      "loss": 0.1686,
      "step": 7430
    },
    {
      "epoch": 4.399940837154267,
      "grad_norm": 0.22442936897277832,
      "learning_rate": 2.410408042578356e-05,
      "loss": 0.1556,
      "step": 7440
    },
    {
      "epoch": 4.405857121727555,
      "grad_norm": 0.20879776775836945,
      "learning_rate": 2.38675340035482e-05,
      "loss": 0.1473,
      "step": 7450
    },
    {
      "epoch": 4.411773406300843,
      "grad_norm": 0.2418179214000702,
      "learning_rate": 2.3630987581312832e-05,
      "loss": 0.1542,
      "step": 7460
    },
    {
      "epoch": 4.417689690874131,
      "grad_norm": 0.19850753247737885,
      "learning_rate": 2.339444115907747e-05,
      "loss": 0.1548,
      "step": 7470
    },
    {
      "epoch": 4.423605975447419,
      "grad_norm": 0.2445548176765442,
      "learning_rate": 2.3157894736842107e-05,
      "loss": 0.1579,
      "step": 7480
    },
    {
      "epoch": 4.429522260020707,
      "grad_norm": 0.2094845324754715,
      "learning_rate": 2.292134831460674e-05,
      "loss": 0.1437,
      "step": 7490
    },
    {
      "epoch": 4.435438544593995,
      "grad_norm": 0.28073224425315857,
      "learning_rate": 2.2684801892371378e-05,
      "loss": 0.1598,
      "step": 7500
    },
    {
      "epoch": 4.441354829167283,
      "grad_norm": 0.22543679177761078,
      "learning_rate": 2.2448255470136015e-05,
      "loss": 0.1678,
      "step": 7510
    },
    {
      "epoch": 4.447271113740571,
      "grad_norm": 0.21308545768260956,
      "learning_rate": 2.2211709047900652e-05,
      "loss": 0.1352,
      "step": 7520
    },
    {
      "epoch": 4.453187398313859,
      "grad_norm": 0.24119937419891357,
      "learning_rate": 2.197516262566529e-05,
      "loss": 0.1442,
      "step": 7530
    },
    {
      "epoch": 4.459103682887147,
      "grad_norm": 0.23285947740077972,
      "learning_rate": 2.1738616203429924e-05,
      "loss": 0.1523,
      "step": 7540
    },
    {
      "epoch": 4.465019967460435,
      "grad_norm": 0.1992964893579483,
      "learning_rate": 2.150206978119456e-05,
      "loss": 0.1429,
      "step": 7550
    },
    {
      "epoch": 4.470936252033723,
      "grad_norm": 0.259722501039505,
      "learning_rate": 2.1265523358959198e-05,
      "loss": 0.1552,
      "step": 7560
    },
    {
      "epoch": 4.47685253660701,
      "grad_norm": 0.2702496647834778,
      "learning_rate": 2.1028976936723832e-05,
      "loss": 0.1602,
      "step": 7570
    },
    {
      "epoch": 4.482768821180299,
      "grad_norm": 0.2598971128463745,
      "learning_rate": 2.079243051448847e-05,
      "loss": 0.1575,
      "step": 7580
    },
    {
      "epoch": 4.488685105753587,
      "grad_norm": 0.25586169958114624,
      "learning_rate": 2.0555884092253106e-05,
      "loss": 0.1638,
      "step": 7590
    },
    {
      "epoch": 4.494601390326875,
      "grad_norm": 0.24399696290493011,
      "learning_rate": 2.031933767001774e-05,
      "loss": 0.146,
      "step": 7600
    },
    {
      "epoch": 4.500517674900163,
      "grad_norm": 0.21213319897651672,
      "learning_rate": 2.0082791247782378e-05,
      "loss": 0.1515,
      "step": 7610
    },
    {
      "epoch": 4.50643395947345,
      "grad_norm": 0.2832062542438507,
      "learning_rate": 1.9846244825547015e-05,
      "loss": 0.1527,
      "step": 7620
    },
    {
      "epoch": 4.512350244046739,
      "grad_norm": 0.2306273728609085,
      "learning_rate": 1.9609698403311652e-05,
      "loss": 0.1531,
      "step": 7630
    },
    {
      "epoch": 4.5182665286200265,
      "grad_norm": 0.24796725809574127,
      "learning_rate": 1.9373151981076286e-05,
      "loss": 0.1539,
      "step": 7640
    },
    {
      "epoch": 4.524182813193315,
      "grad_norm": 0.2505396902561188,
      "learning_rate": 1.9136605558840923e-05,
      "loss": 0.1486,
      "step": 7650
    },
    {
      "epoch": 4.530099097766603,
      "grad_norm": 0.24852582812309265,
      "learning_rate": 1.890005913660556e-05,
      "loss": 0.1474,
      "step": 7660
    },
    {
      "epoch": 4.53601538233989,
      "grad_norm": 0.25723326206207275,
      "learning_rate": 1.8663512714370194e-05,
      "loss": 0.1635,
      "step": 7670
    },
    {
      "epoch": 4.541931666913179,
      "grad_norm": 0.20864169299602509,
      "learning_rate": 1.842696629213483e-05,
      "loss": 0.1713,
      "step": 7680
    },
    {
      "epoch": 4.5478479514864665,
      "grad_norm": 0.25772354006767273,
      "learning_rate": 1.819041986989947e-05,
      "loss": 0.1541,
      "step": 7690
    },
    {
      "epoch": 4.553764236059754,
      "grad_norm": 0.19664356112480164,
      "learning_rate": 1.7953873447664106e-05,
      "loss": 0.1517,
      "step": 7700
    },
    {
      "epoch": 4.559680520633043,
      "grad_norm": 0.23248639702796936,
      "learning_rate": 1.771732702542874e-05,
      "loss": 0.163,
      "step": 7710
    },
    {
      "epoch": 4.56559680520633,
      "grad_norm": 0.21295267343521118,
      "learning_rate": 1.7480780603193377e-05,
      "loss": 0.1523,
      "step": 7720
    },
    {
      "epoch": 4.571513089779619,
      "grad_norm": 0.25950318574905396,
      "learning_rate": 1.7244234180958014e-05,
      "loss": 0.1539,
      "step": 7730
    },
    {
      "epoch": 4.577429374352906,
      "grad_norm": 0.2692018449306488,
      "learning_rate": 1.7007687758722652e-05,
      "loss": 0.1664,
      "step": 7740
    },
    {
      "epoch": 4.583345658926194,
      "grad_norm": 0.29273584485054016,
      "learning_rate": 1.6771141336487286e-05,
      "loss": 0.1521,
      "step": 7750
    },
    {
      "epoch": 4.589261943499483,
      "grad_norm": 0.22429849207401276,
      "learning_rate": 1.6534594914251923e-05,
      "loss": 0.1519,
      "step": 7760
    },
    {
      "epoch": 4.59517822807277,
      "grad_norm": 0.25596633553504944,
      "learning_rate": 1.629804849201656e-05,
      "loss": 0.1437,
      "step": 7770
    },
    {
      "epoch": 4.601094512646059,
      "grad_norm": 0.199209526181221,
      "learning_rate": 1.6061502069781194e-05,
      "loss": 0.1483,
      "step": 7780
    },
    {
      "epoch": 4.607010797219346,
      "grad_norm": 0.27402180433273315,
      "learning_rate": 1.582495564754583e-05,
      "loss": 0.1558,
      "step": 7790
    },
    {
      "epoch": 4.612927081792634,
      "grad_norm": 0.2638075053691864,
      "learning_rate": 1.558840922531047e-05,
      "loss": 0.1541,
      "step": 7800
    },
    {
      "epoch": 4.6188433663659225,
      "grad_norm": 0.2430209368467331,
      "learning_rate": 1.5351862803075102e-05,
      "loss": 0.1554,
      "step": 7810
    },
    {
      "epoch": 4.62475965093921,
      "grad_norm": 0.31262117624282837,
      "learning_rate": 1.5115316380839741e-05,
      "loss": 0.1562,
      "step": 7820
    },
    {
      "epoch": 4.630675935512498,
      "grad_norm": 0.22502876818180084,
      "learning_rate": 1.4878769958604377e-05,
      "loss": 0.1497,
      "step": 7830
    },
    {
      "epoch": 4.636592220085786,
      "grad_norm": 0.2496362179517746,
      "learning_rate": 1.4642223536369012e-05,
      "loss": 0.1546,
      "step": 7840
    },
    {
      "epoch": 4.642508504659074,
      "grad_norm": 0.21871668100357056,
      "learning_rate": 1.440567711413365e-05,
      "loss": 0.1487,
      "step": 7850
    },
    {
      "epoch": 4.6484247892323625,
      "grad_norm": 0.21630316972732544,
      "learning_rate": 1.4169130691898285e-05,
      "loss": 0.152,
      "step": 7860
    },
    {
      "epoch": 4.65434107380565,
      "grad_norm": 0.2881043553352356,
      "learning_rate": 1.393258426966292e-05,
      "loss": 0.1548,
      "step": 7870
    },
    {
      "epoch": 4.660257358378938,
      "grad_norm": 0.2292417287826538,
      "learning_rate": 1.369603784742756e-05,
      "loss": 0.1489,
      "step": 7880
    },
    {
      "epoch": 4.666173642952226,
      "grad_norm": 0.26031437516212463,
      "learning_rate": 1.3459491425192194e-05,
      "loss": 0.1516,
      "step": 7890
    },
    {
      "epoch": 4.672089927525514,
      "grad_norm": 0.23947297036647797,
      "learning_rate": 1.322294500295683e-05,
      "loss": 0.1451,
      "step": 7900
    },
    {
      "epoch": 4.678006212098802,
      "grad_norm": 0.3121877908706665,
      "learning_rate": 1.2986398580721468e-05,
      "loss": 0.1563,
      "step": 7910
    },
    {
      "epoch": 4.68392249667209,
      "grad_norm": 0.2451169788837433,
      "learning_rate": 1.2749852158486104e-05,
      "loss": 0.1614,
      "step": 7920
    },
    {
      "epoch": 4.689838781245378,
      "grad_norm": 0.2675584554672241,
      "learning_rate": 1.2513305736250741e-05,
      "loss": 0.15,
      "step": 7930
    },
    {
      "epoch": 4.695755065818666,
      "grad_norm": 0.24129122495651245,
      "learning_rate": 1.2276759314015377e-05,
      "loss": 0.1426,
      "step": 7940
    },
    {
      "epoch": 4.701671350391954,
      "grad_norm": 0.2453509271144867,
      "learning_rate": 1.2040212891780014e-05,
      "loss": 0.1581,
      "step": 7950
    },
    {
      "epoch": 4.7075876349652415,
      "grad_norm": 0.2431812733411789,
      "learning_rate": 1.1803666469544648e-05,
      "loss": 0.1368,
      "step": 7960
    },
    {
      "epoch": 4.71350391953853,
      "grad_norm": 0.27041468024253845,
      "learning_rate": 1.1567120047309285e-05,
      "loss": 0.1593,
      "step": 7970
    },
    {
      "epoch": 4.719420204111818,
      "grad_norm": 0.22581903636455536,
      "learning_rate": 1.1330573625073922e-05,
      "loss": 0.1574,
      "step": 7980
    },
    {
      "epoch": 4.725336488685106,
      "grad_norm": 0.19369789958000183,
      "learning_rate": 1.1094027202838558e-05,
      "loss": 0.1506,
      "step": 7990
    },
    {
      "epoch": 4.731252773258394,
      "grad_norm": 0.27056604623794556,
      "learning_rate": 1.0857480780603193e-05,
      "loss": 0.1477,
      "step": 8000
    },
    {
      "epoch": 4.737169057831681,
      "grad_norm": 0.25149431824684143,
      "learning_rate": 1.062093435836783e-05,
      "loss": 0.1417,
      "step": 8010
    },
    {
      "epoch": 4.74308534240497,
      "grad_norm": 0.22005166113376617,
      "learning_rate": 1.0384387936132466e-05,
      "loss": 0.151,
      "step": 8020
    },
    {
      "epoch": 4.749001626978258,
      "grad_norm": 0.23262766003608704,
      "learning_rate": 1.0147841513897102e-05,
      "loss": 0.1549,
      "step": 8030
    },
    {
      "epoch": 4.754917911551546,
      "grad_norm": 0.24763888120651245,
      "learning_rate": 9.911295091661739e-06,
      "loss": 0.1572,
      "step": 8040
    },
    {
      "epoch": 4.760834196124834,
      "grad_norm": 0.22085948288440704,
      "learning_rate": 9.674748669426376e-06,
      "loss": 0.1582,
      "step": 8050
    },
    {
      "epoch": 4.766750480698121,
      "grad_norm": 0.26725679636001587,
      "learning_rate": 9.438202247191012e-06,
      "loss": 0.1529,
      "step": 8060
    },
    {
      "epoch": 4.77266676527141,
      "grad_norm": 0.30580103397369385,
      "learning_rate": 9.201655824955647e-06,
      "loss": 0.1761,
      "step": 8070
    },
    {
      "epoch": 4.7785830498446975,
      "grad_norm": 0.2617846429347992,
      "learning_rate": 8.965109402720285e-06,
      "loss": 0.1627,
      "step": 8080
    },
    {
      "epoch": 4.784499334417985,
      "grad_norm": 0.220133975148201,
      "learning_rate": 8.72856298048492e-06,
      "loss": 0.1487,
      "step": 8090
    },
    {
      "epoch": 4.790415618991274,
      "grad_norm": 0.3055367171764374,
      "learning_rate": 8.492016558249557e-06,
      "loss": 0.1556,
      "step": 8100
    },
    {
      "epoch": 4.796331903564561,
      "grad_norm": 0.22133086621761322,
      "learning_rate": 8.255470136014193e-06,
      "loss": 0.1443,
      "step": 8110
    },
    {
      "epoch": 4.80224818813785,
      "grad_norm": 0.23907068371772766,
      "learning_rate": 8.01892371377883e-06,
      "loss": 0.1546,
      "step": 8120
    },
    {
      "epoch": 4.8081644727111374,
      "grad_norm": 0.24348662793636322,
      "learning_rate": 7.782377291543466e-06,
      "loss": 0.1542,
      "step": 8130
    },
    {
      "epoch": 4.814080757284425,
      "grad_norm": 0.2542770504951477,
      "learning_rate": 7.545830869308101e-06,
      "loss": 0.1566,
      "step": 8140
    },
    {
      "epoch": 4.819997041857714,
      "grad_norm": 0.26161521673202515,
      "learning_rate": 7.309284447072738e-06,
      "loss": 0.1469,
      "step": 8150
    },
    {
      "epoch": 4.825913326431001,
      "grad_norm": 0.2220713198184967,
      "learning_rate": 7.072738024837375e-06,
      "loss": 0.142,
      "step": 8160
    },
    {
      "epoch": 4.83182961100429,
      "grad_norm": 0.25124868750572205,
      "learning_rate": 6.836191602602011e-06,
      "loss": 0.1606,
      "step": 8170
    },
    {
      "epoch": 4.837745895577577,
      "grad_norm": 0.20401901006698608,
      "learning_rate": 6.599645180366647e-06,
      "loss": 0.151,
      "step": 8180
    },
    {
      "epoch": 4.843662180150865,
      "grad_norm": 0.24272628128528595,
      "learning_rate": 6.363098758131283e-06,
      "loss": 0.1559,
      "step": 8190
    },
    {
      "epoch": 4.8495784647241535,
      "grad_norm": 0.2756144106388092,
      "learning_rate": 6.12655233589592e-06,
      "loss": 0.1711,
      "step": 8200
    },
    {
      "epoch": 4.855494749297441,
      "grad_norm": 0.26865169405937195,
      "learning_rate": 5.890005913660556e-06,
      "loss": 0.1575,
      "step": 8210
    },
    {
      "epoch": 4.861411033870729,
      "grad_norm": 0.24467529356479645,
      "learning_rate": 5.653459491425193e-06,
      "loss": 0.1471,
      "step": 8220
    },
    {
      "epoch": 4.867327318444017,
      "grad_norm": 0.276353120803833,
      "learning_rate": 5.416913069189829e-06,
      "loss": 0.1547,
      "step": 8230
    },
    {
      "epoch": 4.873243603017305,
      "grad_norm": 0.2194579839706421,
      "learning_rate": 5.1803666469544654e-06,
      "loss": 0.1533,
      "step": 8240
    },
    {
      "epoch": 4.8791598875905935,
      "grad_norm": 0.27589482069015503,
      "learning_rate": 4.943820224719101e-06,
      "loss": 0.154,
      "step": 8250
    },
    {
      "epoch": 4.885076172163881,
      "grad_norm": 0.2599813640117645,
      "learning_rate": 4.707273802483737e-06,
      "loss": 0.1597,
      "step": 8260
    },
    {
      "epoch": 4.890992456737169,
      "grad_norm": 0.28617027401924133,
      "learning_rate": 4.470727380248374e-06,
      "loss": 0.149,
      "step": 8270
    },
    {
      "epoch": 4.896908741310457,
      "grad_norm": 0.31148990988731384,
      "learning_rate": 4.23418095801301e-06,
      "loss": 0.1688,
      "step": 8280
    },
    {
      "epoch": 4.902825025883745,
      "grad_norm": 0.25847914814949036,
      "learning_rate": 3.997634535777647e-06,
      "loss": 0.1604,
      "step": 8290
    },
    {
      "epoch": 4.908741310457033,
      "grad_norm": 0.21467001736164093,
      "learning_rate": 3.7610881135422826e-06,
      "loss": 0.1534,
      "step": 8300
    },
    {
      "epoch": 4.914657595030321,
      "grad_norm": 0.24198158085346222,
      "learning_rate": 3.5245416913069195e-06,
      "loss": 0.1432,
      "step": 8310
    },
    {
      "epoch": 4.920573879603609,
      "grad_norm": 0.2312328815460205,
      "learning_rate": 3.2879952690715554e-06,
      "loss": 0.1502,
      "step": 8320
    },
    {
      "epoch": 4.926490164176897,
      "grad_norm": 0.22883369028568268,
      "learning_rate": 3.0514488468361914e-06,
      "loss": 0.1518,
      "step": 8330
    },
    {
      "epoch": 4.932406448750185,
      "grad_norm": 0.25152674317359924,
      "learning_rate": 2.814902424600828e-06,
      "loss": 0.1568,
      "step": 8340
    },
    {
      "epoch": 4.9383227333234725,
      "grad_norm": 0.21888670325279236,
      "learning_rate": 2.5783560023654642e-06,
      "loss": 0.1522,
      "step": 8350
    },
    {
      "epoch": 4.944239017896761,
      "grad_norm": 0.22324790060520172,
      "learning_rate": 2.3418095801301006e-06,
      "loss": 0.1512,
      "step": 8360
    },
    {
      "epoch": 4.950155302470049,
      "grad_norm": 0.27006620168685913,
      "learning_rate": 2.105263157894737e-06,
      "loss": 0.1592,
      "step": 8370
    },
    {
      "epoch": 4.956071587043337,
      "grad_norm": 0.2628711760044098,
      "learning_rate": 1.8687167356593733e-06,
      "loss": 0.1528,
      "step": 8380
    },
    {
      "epoch": 4.961987871616625,
      "grad_norm": 0.24369870126247406,
      "learning_rate": 1.6321703134240095e-06,
      "loss": 0.1472,
      "step": 8390
    },
    {
      "epoch": 4.967904156189912,
      "grad_norm": 0.253816157579422,
      "learning_rate": 1.3956238911886459e-06,
      "loss": 0.1474,
      "step": 8400
    }
  ],
  "logging_steps": 10,
  "max_steps": 8455,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.0988169030286705e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
